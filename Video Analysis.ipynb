{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244a58fb",
   "metadata": {},
   "source": [
    "# What Makes A Trending Animal Video?\n",
    "\n",
    "This notebook focuses on answering that question in the most appropriate way possible, with regression, neural networks, transformers, and the like. Each section is written so you can run the script yourself, although this isn't recommended as it could take gigabytes of storage and hours of processing to deal with all of the data and training.\n",
    "\n",
    "To simplify the sections, they are below:\n",
    "\n",
    "0. Base installs for videos/datascience stuff\n",
    "1. Grab all category information for YouTube videos\n",
    "2. Retrieve data from the CSV file, download all necessary pieces of data (thumbnails, mp4's, descriptions, etc)\n",
    "3. Preprocessing the data\n",
    "4. Modelling and evaluating the data (with evaluations after the definitions/trainings)\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cda53",
   "metadata": {},
   "source": [
    "## 0) Core setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbfed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q clean-text\n",
    "%pip install -q pytubefix\n",
    "%pip install -q opendatasets\n",
    "%pip install -q pandas\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from cleantext import clean\n",
    "from pytubefix import YouTube, exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2e3d7",
   "metadata": {},
   "source": [
    "## Here is a quick thing for actually running this code, where you will have to download the data and put it into a specific folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea88267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the dataset\n",
    "!mkdir ./DS\n",
    "!wget -q https://huggingface.co/datasets/bagelsause/youtube_dataset/resolve/main/US_category_id.json -O ./DS/US_category_id.json\n",
    "!wget -q https://huggingface.co/datasets/bagelsause/youtube_dataset/resolve/main/US_youtube_trending_data.csv.zip -O ./DS/US_youtube_trending_data.csv.zip\n",
    "\n",
    "#unzip the dataset\n",
    "!unzip -q ./DS/US_youtube_trending_data.csv.zip -d ./DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3d59b",
   "metadata": {},
   "source": [
    "## 1) Grab all category information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a75e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignable Category: Film & Animation | ID: 1\n",
      "Assignable Category: Autos & Vehicles | ID: 2\n",
      "Assignable Category: Music | ID: 10\n",
      "Assignable Category: Pets & Animals | ID: 15\n",
      "Assignable Category: Sports | ID: 17\n",
      "Assignable Category: Travel & Events | ID: 19\n",
      "Assignable Category: Gaming | ID: 20\n",
      "Assignable Category: People & Blogs | ID: 22\n",
      "Assignable Category: Comedy | ID: 23\n",
      "Assignable Category: Entertainment | ID: 24\n",
      "Assignable Category: News & Politics | ID: 25\n",
      "Assignable Category: Howto & Style | ID: 26\n",
      "Assignable Category: Education | ID: 27\n",
      "Assignable Category: Science & Technology | ID: 28\n",
      "Assignable Category: Nonprofits & Activism | ID: 29\n"
     ]
    }
   ],
   "source": [
    "file = open('./DS/US_category_id.json')\n",
    "categories = json.load(file)\n",
    "\n",
    "fullCategories = {}\n",
    "for i in categories['items']:\n",
    "    if (i['snippet']['assignable']):\n",
    "        title = i['snippet']['title']\n",
    "        ID = i['id']\n",
    "        print(\"Assignable Category: \" + title + \" | ID: \" + ID)\n",
    "        fullCategories[title] = ID\n",
    "\n",
    "#this code should grab all of the possible category id's available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949a561",
   "metadata": {},
   "source": [
    "## 2.1) Get Subset of Cat Group, Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50503bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "animalID = int(fullCategories['Pets & Animals'])\n",
    "print(animalID) #specifically the \"pets and animals\" ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2791ec04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>263450</td>\n",
       "      <td>22669</td>\n",
       "      <td>4591</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>280138</td>\n",
       "      <td>23626</td>\n",
       "      <td>4809</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>291192</td>\n",
       "      <td>24150</td>\n",
       "      <td>4954</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>Biagyb7AcK8</td>\n",
       "      <td>True Facts: The Hummingbird Warrior</td>\n",
       "      <td>[None]</td>\n",
       "      <td>505104</td>\n",
       "      <td>53244</td>\n",
       "      <td>2866</td>\n",
       "      <td>https://i.ytimg.com/vi/Biagyb7AcK8/default.jpg</td>\n",
       "      <td>Not a moth.SHIRTS: https://ze-true-store.mysho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>298962</td>\n",
       "      <td>24492</td>\n",
       "      <td>4989</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                                title  \\\n",
       "104  3LqQB4HV8qg      I rescued my cat from the sewer   \n",
       "331  3LqQB4HV8qg      I rescued my cat from the sewer   \n",
       "556  3LqQB4HV8qg      I rescued my cat from the sewer   \n",
       "614  Biagyb7AcK8  True Facts: The Hummingbird Warrior   \n",
       "783  3LqQB4HV8qg      I rescued my cat from the sewer   \n",
       "\n",
       "                                                  tags  view_count  likes  \\\n",
       "104  joey|graceffa|joey graceffa|pregnant cat|rescu...      263450  22669   \n",
       "331  joey|graceffa|joey graceffa|pregnant cat|rescu...      280138  23626   \n",
       "556  joey|graceffa|joey graceffa|pregnant cat|rescu...      291192  24150   \n",
       "614                                             [None]      505104  53244   \n",
       "783  joey|graceffa|joey graceffa|pregnant cat|rescu...      298962  24492   \n",
       "\n",
       "     comment_count                                  thumbnail_link  \\\n",
       "104           4591  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "331           4809  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "556           4954  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "614           2866  https://i.ytimg.com/vi/Biagyb7AcK8/default.jpg   \n",
       "783           4989  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "\n",
       "                                           description  \n",
       "104  I rescued my cat after she got stuck in the se...  \n",
       "331  I rescued my cat after she got stuck in the se...  \n",
       "556  I rescued my cat after she got stuck in the se...  \n",
       "614  Not a moth.SHIRTS: https://ze-true-store.mysho...  \n",
       "783  I rescued my cat after she got stuck in the se...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videodf = pd.read_csv('./DS/US_youtube_trending_data.csv')\n",
    "specificMetrics = ['video_id', 'title', 'tags', 'view_count', 'likes', 'comment_count', 'thumbnail_link', 'description']\n",
    "videodf = videodf[videodf['categoryId'] == animalID][specificMetrics]\n",
    "videodf.head()\n",
    "\n",
    "#get all videoid's titles, tags, viewcounts, likes, comment_counts, thumbnail links, and descriptions for each \"pets and animals\" video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5c6c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>263450</td>\n",
       "      <td>22669</td>\n",
       "      <td>4591</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>280138</td>\n",
       "      <td>23626</td>\n",
       "      <td>4809</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>291192</td>\n",
       "      <td>24150</td>\n",
       "      <td>4954</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>298962</td>\n",
       "      <td>24492</td>\n",
       "      <td>4989</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                            title  \\\n",
       "104  3LqQB4HV8qg  I rescued my cat from the sewer   \n",
       "331  3LqQB4HV8qg  I rescued my cat from the sewer   \n",
       "556  3LqQB4HV8qg  I rescued my cat from the sewer   \n",
       "783  3LqQB4HV8qg  I rescued my cat from the sewer   \n",
       "\n",
       "                                                  tags  view_count  likes  \\\n",
       "104  joey|graceffa|joey graceffa|pregnant cat|rescu...      263450  22669   \n",
       "331  joey|graceffa|joey graceffa|pregnant cat|rescu...      280138  23626   \n",
       "556  joey|graceffa|joey graceffa|pregnant cat|rescu...      291192  24150   \n",
       "783  joey|graceffa|joey graceffa|pregnant cat|rescu...      298962  24492   \n",
       "\n",
       "     comment_count                                  thumbnail_link  \\\n",
       "104           4591  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "331           4809  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "556           4954  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "783           4989  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "\n",
       "                                           description  \n",
       "104  I rescued my cat after she got stuck in the se...  \n",
       "331  I rescued my cat after she got stuck in the se...  \n",
       "556  I rescued my cat after she got stuck in the se...  \n",
       "783  I rescued my cat after she got stuck in the se...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videodf[videodf['video_id'] == \"3LqQB4HV8qg\"]\n",
    "#example video ID, it's trending multiple times, so it was given multiple entries, we don't care about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755ad3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-ZzjDXc-pl8</td>\n",
       "      <td>Crane Introduces His Babies To His Human Best ...</td>\n",
       "      <td>animal video|animals|the dodo|Animal Rescue|do...</td>\n",
       "      <td>5162730</td>\n",
       "      <td>50938</td>\n",
       "      <td>2119</td>\n",
       "      <td>https://i.ytimg.com/vi/-ZzjDXc-pl8/default.jpg</td>\n",
       "      <td>Crane knocks on his human best friend's door e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04KxKOCZGLo</td>\n",
       "      <td>Pretending to Put My Dog on a Diet</td>\n",
       "      <td>pretending to put my dog on a diet|dog diet ch...</td>\n",
       "      <td>2414647</td>\n",
       "      <td>60658</td>\n",
       "      <td>4462</td>\n",
       "      <td>https://i.ytimg.com/vi/04KxKOCZGLo/default.jpg</td>\n",
       "      <td>Pretending to Put My Dog on a DietToday we pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>085oabjiS8U</td>\n",
       "      <td>Brave man rescues exhausted coyote</td>\n",
       "      <td>[None]</td>\n",
       "      <td>8429238</td>\n",
       "      <td>36292</td>\n",
       "      <td>3585</td>\n",
       "      <td>https://i.ytimg.com/vi/085oabjiS8U/default.jpg</td>\n",
       "      <td>When all hope is lost, the greatest gift is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0C2K9jvqtXs</td>\n",
       "      <td>True Facts: The Curious Adaptations of Sharks</td>\n",
       "      <td>[None]</td>\n",
       "      <td>1001792</td>\n",
       "      <td>76622</td>\n",
       "      <td>2923</td>\n",
       "      <td>https://i.ytimg.com/vi/0C2K9jvqtXs/default.jpg</td>\n",
       "      <td>Go to https://brilliant.org/zefrank to get a 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ka_RArXke8</td>\n",
       "      <td>Dogs Vs Cats | Tofu the Corgi | #shorts</td>\n",
       "      <td>shorts|corgi|dog life|cat life</td>\n",
       "      <td>1818865</td>\n",
       "      <td>130646</td>\n",
       "      <td>1331</td>\n",
       "      <td>https://i.ytimg.com/vi/0ka_RArXke8/default.jpg</td>\n",
       "      <td>Anybody else agree? #shortsFOLLOW TOFU THE COR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  -ZzjDXc-pl8  Crane Introduces His Babies To His Human Best ...   \n",
       "1  04KxKOCZGLo                 Pretending to Put My Dog on a Diet   \n",
       "2  085oabjiS8U                 Brave man rescues exhausted coyote   \n",
       "3  0C2K9jvqtXs      True Facts: The Curious Adaptations of Sharks   \n",
       "4  0ka_RArXke8            Dogs Vs Cats | Tofu the Corgi | #shorts   \n",
       "\n",
       "                                                tags  view_count   likes  \\\n",
       "0  animal video|animals|the dodo|Animal Rescue|do...     5162730   50938   \n",
       "1  pretending to put my dog on a diet|dog diet ch...     2414647   60658   \n",
       "2                                             [None]     8429238   36292   \n",
       "3                                             [None]     1001792   76622   \n",
       "4                     shorts|corgi|dog life|cat life     1818865  130646   \n",
       "\n",
       "   comment_count                                  thumbnail_link  \\\n",
       "0           2119  https://i.ytimg.com/vi/-ZzjDXc-pl8/default.jpg   \n",
       "1           4462  https://i.ytimg.com/vi/04KxKOCZGLo/default.jpg   \n",
       "2           3585  https://i.ytimg.com/vi/085oabjiS8U/default.jpg   \n",
       "3           2923  https://i.ytimg.com/vi/0C2K9jvqtXs/default.jpg   \n",
       "4           1331  https://i.ytimg.com/vi/0ka_RArXke8/default.jpg   \n",
       "\n",
       "                                         description  \n",
       "0  Crane knocks on his human best friend's door e...  \n",
       "1  Pretending to Put My Dog on a DietToday we pre...  \n",
       "2  When all hope is lost, the greatest gift is a ...  \n",
       "3  Go to https://brilliant.org/zefrank to get a 3...  \n",
       "4  Anybody else agree? #shortsFOLLOW TOFU THE COR...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort by video id (group common video id's together) then sort by viewcount\n",
    "df_sorted = videodf.sort_values(by=['video_id', 'view_count'], ascending=[False, False])\n",
    "\n",
    "#keep first row of each video_id group (the one with the highest count)\n",
    "cleaneddf = df_sorted.groupby('video_id').first().reset_index()\n",
    "\n",
    "cleaneddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e5b63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3LqQB4HV8qg</td>\n",
       "      <td>I rescued my cat from the sewer</td>\n",
       "      <td>joey|graceffa|joey graceffa|pregnant cat|rescu...</td>\n",
       "      <td>298962</td>\n",
       "      <td>24492</td>\n",
       "      <td>4989</td>\n",
       "      <td>https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg</td>\n",
       "      <td>I rescued my cat after she got stuck in the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id                            title  \\\n",
       "13  3LqQB4HV8qg  I rescued my cat from the sewer   \n",
       "\n",
       "                                                 tags  view_count  likes  \\\n",
       "13  joey|graceffa|joey graceffa|pregnant cat|rescu...      298962  24492   \n",
       "\n",
       "    comment_count                                  thumbnail_link  \\\n",
       "13           4989  https://i.ytimg.com/vi/3LqQB4HV8qg/default.jpg   \n",
       "\n",
       "                                          description  \n",
       "13  I rescued my cat after she got stuck in the se...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaneddf[cleaneddf['video_id'] == \"3LqQB4HV8qg\"]\n",
    "#looking at it again, only one entry exists for that video, and it's the entry that was last grabbed (highest viewcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f049f",
   "metadata": {},
   "source": [
    "## 2.2.1) Post-cleaning, fetch and store all thumbnails&descriptions of videos\n",
    "Overarching \"YT WORK\" folder contains all files necessary, inside \"DATA\" folder, we create a bunch of folders with the associated \"video_id's\" that contains the thumbnails, mp4's, & descriptions for the video. First portion focuses on retrieving thumbnails and descriptions (present within the dataframe itself), second focuses on the mp4's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"./DATA/\"\n",
    "\n",
    "for index, video in cleaneddf.iterrows():\n",
    "    videoPath = datapath + video[\"video_id\"]\n",
    "    #make folder if it doesn't already exist\n",
    "    if not os.path.exists(videoPath):\n",
    "        os.makedirs(videoPath)\n",
    "    \n",
    "    #YT Work/Data/-ZzjDXc-pl8/\n",
    "    videoPath += \"/\"\n",
    "    \n",
    "    #save thumbnail as image file in folder\n",
    "    thumbnail = \"http://img.youtube.com/vi/\" + video[\"video_id\"] + \"/0.jpg\"\n",
    "    \n",
    "    img_data = requests.get(thumbnail).content\n",
    "    with open(videoPath + 'image.jpg', 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "    \n",
    "    #save description as a .txt file in the folder\n",
    "    f = open(videoPath + \"description.txt\", \"w\")\n",
    "    f.write(clean(video[\"description\"], no_emoji=True))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e942a",
   "metadata": {},
   "source": [
    "## 2.2.2) Fetch MP4's of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2b3bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100% of the dataset, all are located in ./DATA/<VIDEOID>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_completed = 0\n",
    "\n",
    "for index, video in cleaneddf.iterrows():\n",
    "    vidID = video['video_id']\n",
    "    url = \"https://www.youtube.com/watch?v=\" + vidID\n",
    "    \n",
    "    total = cleaneddf.shape[0]\n",
    "    youtube_content = YouTube(url)\n",
    "    outpath = \"./DATA/\"+vidID+\"/\"\n",
    "\n",
    "    #try and get the lowest quality of videos possible for analysis (video files themselves not required, but helpful)\n",
    "    try:\n",
    "        lowestStream = youtube_content.streams.get_by_resolution(\"240p\").download(max_retries=5, output_path=outpath, filename=\"video.mp4\")\n",
    "    except AttributeError:\n",
    "        lowestStream = youtube_content.streams.get_lowest_resolution().download(max_retries=5, output_path=outpath, filename=\"video.mp4\")\n",
    "    except exceptions.VideoUnavailable:\n",
    "        pass #if the video isn't available, we'll skip it.\n",
    "    except Exception as e:\n",
    "        print(f\"\\nEXCEPTION:\" + str(e) + \"| Video ID: \" + str(vidID) + f\"\\n\")\n",
    "\n",
    "    total_completed += 1\n",
    "    \n",
    "    if total_completed % 5 == 0:\n",
    "        print(f\"Complete: {round((total_completed/total)*100, 2)}%\",\n",
    "              end='\\r')\n",
    "\n",
    "print(f\"Completed 100% of the dataset, all are located in ./DATA/<VIDEOID>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e3372-bac9-4f2b-9d10-ec87ca051c63",
   "metadata": {},
   "source": [
    "## 2.3) Final clean of data to only include videos with available thumbnails, descriptions, and mp4's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cb27ef-45a6-4dc5-af76-e2b17d3902f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '3oaf7tLz6O4' is missing files: video.mp4. Deleting folder...\n",
      "Folder '5VQmGKtNX08' is missing files: video.mp4. Deleting folder...\n",
      "Removing entries for video IDs: 3oaf7tLz6O4, 5VQmGKtNX08\n",
      "\n",
      "Total size of the DATA directory: 5.36 GB\n",
      "Cleaned and finalized raw data has been stored.\n"
     ]
    }
   ],
   "source": [
    "#go through each folder in the ./DATA/ directory, and remove all that dont have the three files in there.\n",
    "data_path = \"./DATA/\"\n",
    "required_files = [\"description.txt\", \"image.jpg\", \"video.mp4\"]\n",
    "videos_to_remove = []\n",
    "total_size = 0\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        missing = [file for file in required_files if not os.path.exists(os.path.join(folder_path, file))]\n",
    "\n",
    "        if missing:\n",
    "            print(f\"Folder '{folder}' is missing files: {', '.join(missing)}. Deleting folder...\")\n",
    "            #nuke the folder from storage\n",
    "            for root, dirs, files in os.walk(folder_path, topdown=False):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "                for dir in dirs:\n",
    "                    os.rmdir(os.path.join(root, dir))\n",
    "            os.rmdir(folder_path)\n",
    "            #add it to removal list\n",
    "            videos_to_remove.append(folder)\n",
    "        else:\n",
    "            #calculate size of folder\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "\n",
    "if videos_to_remove:\n",
    "    print(f\"Removing entries for video IDs: {', '.join(videos_to_remove)}\")\n",
    "    cleaneddf = cleaneddf[~cleaneddf['video_id'].isin(videos_to_remove)]\n",
    "\n",
    "print(f\"\\nTotal size of the DATA directory: {total_size / (1024**3):.2f} GB\")\n",
    "\n",
    "#save the finalized cleaned dataframe\n",
    "cleaneddf.to_csv(\"./DATA/cleaned_data.csv\")\n",
    "print(\"Cleaned and finalized raw data has been stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66b400",
   "metadata": {},
   "source": [
    "## 3) Parse each piece of data into something computer-understandable (pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q Pillow\n",
    "%pip install -q transformers\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q torch\n",
    "%pip install -q opencv-python\n",
    "%pip install -q timm\n",
    "%pip install -q ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412be8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bird, 593.2ms\n",
      "Speed: 2.9ms preprocess, 593.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 bowl, 1 dining table, 561.4ms\n",
      "Speed: 1.7ms preprocess, 561.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 sports ball, 628.2ms\n",
      "Speed: 1.9ms preprocess, 628.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 birds, 569.0ms\n",
      "Speed: 1.6ms preprocess, 569.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 578.5ms\n",
      "Speed: 1.8ms preprocess, 578.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 616.7ms\n",
      "Speed: 1.4ms preprocess, 616.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 1 chair, 563.1ms\n",
      "Speed: 1.7ms preprocess, 563.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bear, 1 sports ball, 591.3ms\n",
      "Speed: 1.5ms preprocess, 591.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 1 dog, 653.2ms\n",
      "Speed: 1.8ms preprocess, 653.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 dog, 853.5ms\n",
      "Speed: 2.5ms preprocess, 853.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 831.4ms\n",
      "Speed: 2.5ms preprocess, 831.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 truck, 2 boats, 825.9ms\n",
      "Speed: 2.2ms preprocess, 825.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 bears, 791.2ms\n",
      "Speed: 2.8ms preprocess, 791.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 1 tennis racket, 822.3ms\n",
      "Speed: 2.4ms preprocess, 822.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 boat, 1 dog, 819.1ms\n",
      "Speed: 2.2ms preprocess, 819.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 777.5ms\n",
      "Speed: 3.1ms preprocess, 777.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 birds, 792.6ms\n",
      "Speed: 2.2ms preprocess, 792.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 truck, 808.7ms\n",
      "Speed: 3.1ms preprocess, 808.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 1 dog, 801.2ms\n",
      "Speed: 2.9ms preprocess, 801.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 752.8ms\n",
      "Speed: 2.2ms preprocess, 752.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 714.0ms\n",
      "Speed: 2.8ms preprocess, 714.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 759.1ms\n",
      "Speed: 2.0ms preprocess, 759.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cake, 918.6ms\n",
      "Speed: 2.3ms preprocess, 918.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 chair, 910.7ms\n",
      "Speed: 2.5ms preprocess, 910.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 865.9ms\n",
      "Speed: 2.3ms preprocess, 865.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cake, 2 dining tables, 869.4ms\n",
      "Speed: 2.5ms preprocess, 869.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 toilet, 873.2ms\n",
      "Speed: 2.2ms preprocess, 873.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 793.3ms\n",
      "Speed: 2.3ms preprocess, 793.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 1 dog, 1 bowl, 729.3ms\n",
      "Speed: 2.4ms preprocess, 729.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 765.1ms\n",
      "Speed: 2.4ms preprocess, 765.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 747.5ms\n",
      "Speed: 3.1ms preprocess, 747.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 747.7ms\n",
      "Speed: 2.3ms preprocess, 747.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 784.6ms\n",
      "Speed: 2.2ms preprocess, 784.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bench, 2 dogs, 1 microwave, 851.3ms\n",
      "Speed: 2.4ms preprocess, 851.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cake, 760.3ms\n",
      "Speed: 2.6ms preprocess, 760.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 761.3ms\n",
      "Speed: 3.1ms preprocess, 761.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 horse, 759.4ms\n",
      "Speed: 2.2ms preprocess, 759.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cake, 732.1ms\n",
      "Speed: 2.2ms preprocess, 732.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 dogs, 760.8ms\n",
      "Speed: 2.5ms preprocess, 760.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 721.6ms\n",
      "Speed: 3.1ms preprocess, 721.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 knife, 1 broccoli, 1 couch, 1 dining table, 739.9ms\n",
      "Speed: 2.4ms preprocess, 739.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 bird, 1 vase, 796.3ms\n",
      "Speed: 2.6ms preprocess, 796.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 teddy bear, 719.8ms\n",
      "Speed: 2.1ms preprocess, 719.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 train, 729.8ms\n",
      "Speed: 2.3ms preprocess, 729.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 767.4ms\n",
      "Speed: 2.0ms preprocess, 767.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 1 frisbee, 716.1ms\n",
      "Speed: 2.1ms preprocess, 716.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 740.6ms\n",
      "Speed: 2.4ms preprocess, 740.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 dog, 777.6ms\n",
      "Speed: 8.2ms preprocess, 777.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 712.4ms\n",
      "Speed: 2.2ms preprocess, 712.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 bed, 1 teddy bear, 738.9ms\n",
      "Speed: 2.0ms preprocess, 738.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 1 couch, 1 potted plant, 765.0ms\n",
      "Speed: 2.8ms preprocess, 765.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 cats, 724.2ms\n",
      "Speed: 2.0ms preprocess, 724.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 teddy bear, 732.3ms\n",
      "Speed: 2.0ms preprocess, 732.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 dogs, 757.0ms\n",
      "Speed: 2.3ms preprocess, 757.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bird, 786.8ms\n",
      "Speed: 2.2ms preprocess, 786.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 sheep, 745.7ms\n",
      "Speed: 1.9ms preprocess, 745.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bird, 1 dog, 750.6ms\n",
      "Speed: 2.1ms preprocess, 750.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 732.9ms\n",
      "Speed: 2.1ms preprocess, 732.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 dogs, 717.1ms\n",
      "Speed: 2.4ms preprocess, 717.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 737.2ms\n",
      "Speed: 2.8ms preprocess, 737.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 donut, 719.2ms\n",
      "Speed: 2.2ms preprocess, 719.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 744.9ms\n",
      "Speed: 2.4ms preprocess, 744.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 1 dog, 730.0ms\n",
      "Speed: 2.4ms preprocess, 730.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 717.4ms\n",
      "Speed: 2.2ms preprocess, 717.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 1 potted plant, 757.1ms\n",
      "Speed: 2.4ms preprocess, 757.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bird, 712.6ms\n",
      "Speed: 2.4ms preprocess, 712.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 chair, 1 potted plant, 1 vase, 713.2ms\n",
      "Speed: 2.2ms preprocess, 713.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 train, 769.1ms\n",
      "Speed: 2.2ms preprocess, 769.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 banana, 776.1ms\n",
      "Speed: 2.6ms preprocess, 776.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 704.5ms\n",
      "Speed: 1.9ms preprocess, 704.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 3 traffic lights, 3 horses, 759.0ms\n",
      "Speed: 2.3ms preprocess, 759.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 dogs, 1 sports ball, 1 apple, 1 potted plant, 725.5ms\n",
      "Speed: 2.5ms preprocess, 725.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 726.4ms\n",
      "Speed: 2.4ms preprocess, 726.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 775.9ms\n",
      "Speed: 2.0ms preprocess, 775.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 743.7ms\n",
      "Speed: 2.0ms preprocess, 743.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 735.6ms\n",
      "Speed: 2.0ms preprocess, 735.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 766.1ms\n",
      "Speed: 2.2ms preprocess, 766.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bench, 1 dog, 1 cup, 2 potted plants, 727.4ms\n",
      "Speed: 2.7ms preprocess, 727.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 777.5ms\n",
      "Speed: 2.6ms preprocess, 777.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 6 bowls, 2 couchs, 737.8ms\n",
      "Speed: 2.2ms preprocess, 737.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 birds, 713.0ms\n",
      "Speed: 2.3ms preprocess, 713.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 tie, 2 forks, 1 knife, 1 spoon, 1 donut, 1 couch, 1 potted plant, 1 dining table, 706.1ms\n",
      "Speed: 2.6ms preprocess, 706.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 740.4ms\n",
      "Speed: 1.9ms preprocess, 740.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 1 dog, 733.3ms\n",
      "Speed: 1.9ms preprocess, 733.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 dogs, 755.5ms\n",
      "Speed: 3.0ms preprocess, 755.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 764.0ms\n",
      "Speed: 2.5ms preprocess, 764.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 2 sandwichs, 1 dining table, 771.6ms\n",
      "Speed: 2.2ms preprocess, 771.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 teddy bear, 771.5ms\n",
      "Speed: 2.5ms preprocess, 771.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 dog, 1 potted plant, 1 vase, 718.6ms\n",
      "Speed: 2.2ms preprocess, 718.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 738.7ms\n",
      "Speed: 1.9ms preprocess, 738.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 birds, 755.8ms\n",
      "Speed: 2.8ms preprocess, 755.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 baseball glove, 715.3ms\n",
      "Speed: 2.1ms preprocess, 715.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 boat, 728.5ms\n",
      "Speed: 2.0ms preprocess, 728.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cat, 765.7ms\n",
      "Speed: 2.1ms preprocess, 765.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 725.5ms\n",
      "Speed: 2.9ms preprocess, 725.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 6 birds, 1 bowl, 782.8ms\n",
      "Speed: 2.4ms preprocess, 782.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 dog, 770.0ms\n",
      "Speed: 2.3ms preprocess, 770.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 752.2ms\n",
      "Speed: 2.2ms preprocess, 752.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 736.9ms\n",
      "Speed: 2.0ms preprocess, 736.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 dogs, 814.7ms\n",
      "Speed: 3.2ms preprocess, 814.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 835.6ms\n",
      "Speed: 2.4ms preprocess, 835.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 787.6ms\n",
      "Speed: 2.4ms preprocess, 787.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 788.1ms\n",
      "Speed: 3.5ms preprocess, 788.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cats, 825.3ms\n",
      "Speed: 2.8ms preprocess, 825.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 849.6ms\n",
      "Speed: 2.1ms preprocess, 849.6ms inference, 32.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 dogs, 1 tie, 1 couch, 1 potted plant, 1 bed, 816.6ms\n",
      "Speed: 2.5ms preprocess, 816.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 book, 788.9ms\n",
      "Speed: 2.3ms preprocess, 788.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cup, 1 bowl, 817.7ms\n",
      "Speed: 2.6ms preprocess, 817.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 boats, 1 cat, 786.8ms\n",
      "Speed: 3.2ms preprocess, 786.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 860.0ms\n",
      "Speed: 2.8ms preprocess, 860.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 cup, 844.7ms\n",
      "Speed: 2.6ms preprocess, 844.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 orange, 1 broccoli, 811.7ms\n",
      "Speed: 3.3ms preprocess, 811.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 bed, 909.5ms\n",
      "Speed: 2.7ms preprocess, 909.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 830.2ms\n",
      "Speed: 2.4ms preprocess, 830.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 book, 888.5ms\n",
      "Speed: 2.7ms preprocess, 888.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 dog, 827.5ms\n",
      "Speed: 5.0ms preprocess, 827.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 bed, 831.1ms\n",
      "Speed: 2.2ms preprocess, 831.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 1 bed, 833.8ms\n",
      "Speed: 2.7ms preprocess, 833.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 sink, 1 toothbrush, 789.6ms\n",
      "Speed: 2.3ms preprocess, 789.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 783.3ms\n",
      "Speed: 3.1ms preprocess, 783.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 813.2ms\n",
      "Speed: 2.2ms preprocess, 813.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 1 microwave, 739.6ms\n",
      "Speed: 3.1ms preprocess, 739.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 784.7ms\n",
      "Speed: 2.4ms preprocess, 784.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 757.8ms\n",
      "Speed: 2.5ms preprocess, 757.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 chair, 797.4ms\n",
      "Speed: 2.0ms preprocess, 797.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 dogs, 858.6ms\n",
      "Speed: 2.9ms preprocess, 858.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 potted plant, 1 tv, 764.0ms\n",
      "Speed: 2.2ms preprocess, 764.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 scissors, 753.1ms\n",
      "Speed: 3.4ms preprocess, 753.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 1 suitcase, 784.1ms\n",
      "Speed: 3.5ms preprocess, 784.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 767.1ms\n",
      "Speed: 2.4ms preprocess, 767.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 760.2ms\n",
      "Speed: 2.4ms preprocess, 760.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 785.5ms\n",
      "Speed: 2.2ms preprocess, 785.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 tie, 1 couch, 1 bed, 1 book, 706.0ms\n",
      "Speed: 2.7ms preprocess, 706.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 754.4ms\n",
      "Speed: 2.3ms preprocess, 754.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 elephant, 753.2ms\n",
      "Speed: 2.2ms preprocess, 753.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 dogs, 720.9ms\n",
      "Speed: 2.7ms preprocess, 720.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 753.1ms\n",
      "Speed: 2.3ms preprocess, 753.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 bear, 712.9ms\n",
      "Speed: 2.2ms preprocess, 712.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 732.1ms\n",
      "Speed: 2.0ms preprocess, 732.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 boat, 1 dog, 750.4ms\n",
      "Speed: 2.2ms preprocess, 750.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 cats, 1 dog, 748.1ms\n",
      "Speed: 2.3ms preprocess, 748.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 743.3ms\n",
      "Speed: 3.3ms preprocess, 743.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 apple, 751.2ms\n",
      "Speed: 3.6ms preprocess, 751.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 724.2ms\n",
      "Speed: 1.9ms preprocess, 724.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 734.9ms\n",
      "Speed: 2.2ms preprocess, 734.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 tie, 1 book, 773.5ms\n",
      "Speed: 1.9ms preprocess, 773.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 782.8ms\n",
      "Speed: 2.3ms preprocess, 782.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 717.9ms\n",
      "Speed: 2.8ms preprocess, 717.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 791.7ms\n",
      "Speed: 2.1ms preprocess, 791.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bird, 1 cake, 736.1ms\n",
      "Speed: 2.5ms preprocess, 736.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 737.0ms\n",
      "Speed: 2.3ms preprocess, 737.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 754.2ms\n",
      "Speed: 2.3ms preprocess, 754.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 713.2ms\n",
      "Speed: 2.2ms preprocess, 713.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 2 dogs, 729.8ms\n",
      "Speed: 2.7ms preprocess, 729.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 truck, 2 cats, 749.1ms\n",
      "Speed: 2.2ms preprocess, 749.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 birds, 727.3ms\n",
      "Speed: 2.2ms preprocess, 727.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 chair, 1 dining table, 1 vase, 719.3ms\n",
      "Speed: 2.1ms preprocess, 719.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 762.0ms\n",
      "Speed: 2.4ms preprocess, 762.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 747.1ms\n",
      "Speed: 1.9ms preprocess, 747.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 cats, 1 dog, 747.5ms\n",
      "Speed: 2.1ms preprocess, 747.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 739.1ms\n",
      "Speed: 2.8ms preprocess, 739.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 1 pizza, 720.0ms\n",
      "Speed: 2.3ms preprocess, 720.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 1 pizza, 726.0ms\n",
      "Speed: 2.3ms preprocess, 726.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 dogs, 756.6ms\n",
      "Speed: 2.2ms preprocess, 756.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 733.5ms\n",
      "Speed: 2.1ms preprocess, 733.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 dogs, 763.0ms\n",
      "Speed: 2.1ms preprocess, 763.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 744.4ms\n",
      "Speed: 2.8ms preprocess, 744.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 722.7ms\n",
      "Speed: 2.2ms preprocess, 722.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 760.5ms\n",
      "Speed: 2.4ms preprocess, 760.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 732.3ms\n",
      "Speed: 2.0ms preprocess, 732.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 airplanes, 1 cat, 730.3ms\n",
      "Speed: 2.0ms preprocess, 730.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 truck, 1 cat, 735.6ms\n",
      "Speed: 2.3ms preprocess, 735.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 761.2ms\n",
      "Speed: 2.4ms preprocess, 761.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 711.5ms\n",
      "Speed: 2.2ms preprocess, 711.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 775.9ms\n",
      "Speed: 2.0ms preprocess, 775.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 724.1ms\n",
      "Speed: 1.9ms preprocess, 724.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 2 beds, 764.2ms\n",
      "Speed: 2.0ms preprocess, 764.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 trucks, 740.3ms\n",
      "Speed: 2.2ms preprocess, 740.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bus, 1 truck, 2 dogs, 742.4ms\n",
      "Speed: 2.0ms preprocess, 742.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 2 couchs, 1 potted plant, 750.2ms\n",
      "Speed: 1.9ms preprocess, 750.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 1 dog, 1 book, 751.9ms\n",
      "Speed: 2.4ms preprocess, 751.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 couch, 724.9ms\n",
      "Speed: 2.1ms preprocess, 724.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 broccoli, 731.4ms\n",
      "Speed: 2.3ms preprocess, 731.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 refrigerator, 744.0ms\n",
      "Speed: 2.8ms preprocess, 744.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 3 dogs, 745.5ms\n",
      "Speed: 2.5ms preprocess, 745.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 bird, 756.0ms\n",
      "Speed: 2.4ms preprocess, 756.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 752.1ms\n",
      "Speed: 2.3ms preprocess, 752.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7 horses, 734.7ms\n",
      "Speed: 3.9ms preprocess, 734.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 bed, 735.1ms\n",
      "Speed: 2.2ms preprocess, 735.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 elephants, 762.0ms\n",
      "Speed: 2.3ms preprocess, 762.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 dogs, 730.0ms\n",
      "Speed: 2.2ms preprocess, 730.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Preprocessed data saved to ./DATA/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "#init the models\n",
    "#YOLO v8 for segmentation\n",
    "yolo_model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "#tags and descriptions\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "data_path = \"./DATA/\"\n",
    "output_path = \"./DATA/preprocessed_data.csv\"\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "#for each folder in the data directory\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder+\"/\")\n",
    "    if os.path.isdir(folder_path):\n",
    "        video_id = folder\n",
    "\n",
    "        #grab all view/like counts for prediction purposes\n",
    "        video_data = cleaneddf.loc[cleaneddf[\"video_id\"] == video_id]\n",
    "        if not video_data.empty:\n",
    "            view_count = int(video_data[\"view_count\"].values[0])\n",
    "            like_count = int(video_data[\"likes\"].values[0])\n",
    "        else:\n",
    "            #case where its undefined (never should happen, but just in case)\n",
    "            view_count = 0\n",
    "            like_count = 0\n",
    "\n",
    "        #process the tnumbnail\n",
    "        thumbnail_path = os.path.join(folder_path, \"image.jpg\")\n",
    "        object_counts = {}\n",
    "        if os.path.exists(thumbnail_path):\n",
    "            try:\n",
    "                #load the image\n",
    "                image = Image.open(thumbnail_path).convert(\"RGB\")\n",
    "                results = yolo_model.predict(image)  #run YOLO on the image\n",
    "\n",
    "                detected_objects = [box.cls for box in results[0].boxes]  # Class IDs of detected objects\n",
    "                class_names = [yolo_model.names[int(cls)] for cls in detected_objects]  # Convert IDs to names\n",
    "                object_counts = Counter(class_names)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing thumbnail for {video_id}: {e}\")\n",
    "                object_counts = {}\n",
    "        \n",
    "        #load each object as one-hot encoded of all possible objects\n",
    "        all_possible_objects = list(yolo_model.names.values())\n",
    "        thumbnail_features = {obj: object_counts.get(obj, 0) for obj in all_possible_objects}\n",
    "\n",
    "        #process the description with tf.idf vectorizers\n",
    "        description_path = os.path.join(folder_path, \"description.txt\")\n",
    "        with open(description_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            description = f.read().strip()\n",
    "        #ensure description isnt empty\n",
    "        if description:\n",
    "            try:\n",
    "                description_vector = tfidf_vectorizer.fit_transform([description]).toarray().flatten()\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing description for {video_id}: {e}\")\n",
    "                description_vector = np.zeros(100)\n",
    "        else:\n",
    "            description_vector = np.zeros(100)\n",
    "\n",
    "        #processing the tags\n",
    "        tags = cleaneddf.loc[cleaneddf[\"video_id\"] == video_id, \"tags\"].values\n",
    "        if len(tags) > 0:\n",
    "            tags_vector = tfidf_vectorizer.fit_transform(tags).toarray().mean(axis=0)\n",
    "        else:\n",
    "            #in case we have no tags\n",
    "            tags_vector = np.zeros(100)\n",
    "\n",
    "        #process the video's general features (no in-depth analysis yet)\n",
    "        video_path = os.path.join(folder_path, \"video.mp4\")\n",
    "        if os.path.exists(video_path):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "                duration = frame_count / fps if fps > 0 else 0\n",
    "\n",
    "                #just logging the basic features for now\n",
    "                video_features = [frame_count, fps, duration]\n",
    "\n",
    "                cap.release()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video for {video_id}: {e}\")\n",
    "                video_features = [0, 0, 0]\n",
    "        else:\n",
    "            video_features = [0, 0, 0]\n",
    "\n",
    "        #combine all features into a single row\n",
    "        processed_data.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"view_count\": view_count, #prediction only!!!\n",
    "            \"like_count\": like_count, #prediction only!!!\n",
    "            **thumbnail_features,  #add one-hot encoded object counts as separate columns, rather than raw dictionary\n",
    "            \"description_vector\": description_vector.tolist(),\n",
    "            \"tags_vector\": tags_vector.tolist(),\n",
    "            \"video_features\": video_features\n",
    "        })\n",
    "\n",
    "#convert processed into dataframe and save\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d5a3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>person</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>car</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>airplane</th>\n",
       "      <th>bus</th>\n",
       "      <th>train</th>\n",
       "      <th>...</th>\n",
       "      <th>book</th>\n",
       "      <th>clock</th>\n",
       "      <th>vase</th>\n",
       "      <th>scissors</th>\n",
       "      <th>teddy bear</th>\n",
       "      <th>hair drier</th>\n",
       "      <th>toothbrush</th>\n",
       "      <th>description_vector</th>\n",
       "      <th>tags_vector</th>\n",
       "      <th>video_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-ZzjDXc-pl8</td>\n",
       "      <td>5162730</td>\n",
       "      <td>50938</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.05285164225816899, 0.05285164225816899, 0.0...</td>\n",
       "      <td>[0.24253562503633297, 0.6063390625908325, 0.12...</td>\n",
       "      <td>[5877, 29, 202.6551724137931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04KxKOCZGLo</td>\n",
       "      <td>2414647</td>\n",
       "      <td>60658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0510976130307596, 0.0510976130307596, 0.051...</td>\n",
       "      <td>[0.0582222509739582, 0.0582222509739582, 0.116...</td>\n",
       "      <td>[2863, 30, 95.43333333333334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>085oabjiS8U</td>\n",
       "      <td>8429238</td>\n",
       "      <td>36292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.07088812050083358, 0.07088812050083358, 0.2...</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[5399, 29, 186.17241379310346]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0C2K9jvqtXs</td>\n",
       "      <td>1001792</td>\n",
       "      <td>76622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.018505830254940132, 0.33310494458892237, 0....</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[21636, 23, 940.695652173913]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ka_RArXke8</td>\n",
       "      <td>1818865</td>\n",
       "      <td>130646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10540925533894598, 0.10540925533894598, 0.1...</td>\n",
       "      <td>[0.35355339059327373, 0.35355339059327373, 0.3...</td>\n",
       "      <td>[394, 30, 13.133333333333333]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  view_count  like_count  person  bicycle  car  motorcycle  \\\n",
       "0  -ZzjDXc-pl8     5162730       50938       1        0    0           0   \n",
       "1  04KxKOCZGLo     2414647       60658       0        0    0           0   \n",
       "2  085oabjiS8U     8429238       36292       1        0    0           0   \n",
       "3  0C2K9jvqtXs     1001792       76622       0        0    0           0   \n",
       "4  0ka_RArXke8     1818865      130646       1        0    0           0   \n",
       "\n",
       "   airplane  bus  train  ...  book  clock  vase  scissors  teddy bear  \\\n",
       "0         0    0      0  ...     0      0     0         0           0   \n",
       "1         0    0      0  ...     0      0     0         0           0   \n",
       "2         0    0      0  ...     0      0     0         0           0   \n",
       "3         0    0      0  ...     0      0     0         0           0   \n",
       "4         0    0      0  ...     0      0     0         0           0   \n",
       "\n",
       "   hair drier  toothbrush                                 description_vector  \\\n",
       "0           0           0  [0.05285164225816899, 0.05285164225816899, 0.0...   \n",
       "1           0           0  [0.0510976130307596, 0.0510976130307596, 0.051...   \n",
       "2           0           0  [0.07088812050083358, 0.07088812050083358, 0.2...   \n",
       "3           0           0  [0.018505830254940132, 0.33310494458892237, 0....   \n",
       "4           0           0  [0.10540925533894598, 0.10540925533894598, 0.1...   \n",
       "\n",
       "                                         tags_vector  \\\n",
       "0  [0.24253562503633297, 0.6063390625908325, 0.12...   \n",
       "1  [0.0582222509739582, 0.0582222509739582, 0.116...   \n",
       "2                                              [1.0]   \n",
       "3                                              [1.0]   \n",
       "4  [0.35355339059327373, 0.35355339059327373, 0.3...   \n",
       "\n",
       "                   video_features  \n",
       "0   [5877, 29, 202.6551724137931]  \n",
       "1   [2863, 30, 95.43333333333334]  \n",
       "2  [5399, 29, 186.17241379310346]  \n",
       "3   [21636, 23, 940.695652173913]  \n",
       "4   [394, 30, 13.133333333333333]  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbcc26",
   "metadata": {},
   "source": [
    "# Now that we have all of the data pre-processed, we can now start to work on the actual modelling!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0f5296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>person</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>car</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>airplane</th>\n",
       "      <th>bus</th>\n",
       "      <th>train</th>\n",
       "      <th>...</th>\n",
       "      <th>book</th>\n",
       "      <th>clock</th>\n",
       "      <th>vase</th>\n",
       "      <th>scissors</th>\n",
       "      <th>teddy bear</th>\n",
       "      <th>hair drier</th>\n",
       "      <th>toothbrush</th>\n",
       "      <th>description_vector</th>\n",
       "      <th>tags_vector</th>\n",
       "      <th>video_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-ZzjDXc-pl8</td>\n",
       "      <td>5162730</td>\n",
       "      <td>50938</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.05285164225816899, 0.05285164225816899, 0.0...</td>\n",
       "      <td>[0.24253562503633297, 0.6063390625908325, 0.12...</td>\n",
       "      <td>[5877, 29, 202.6551724137931]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  view_count  like_count  person  bicycle  car  motorcycle  \\\n",
       "0  -ZzjDXc-pl8     5162730       50938       1        0    0           0   \n",
       "\n",
       "   airplane  bus  train  ...  book  clock  vase  scissors  teddy bear  \\\n",
       "0         0    0      0  ...     0      0     0         0           0   \n",
       "\n",
       "   hair drier  toothbrush                                 description_vector  \\\n",
       "0           0           0  [0.05285164225816899, 0.05285164225816899, 0.0...   \n",
       "\n",
       "                                         tags_vector  \\\n",
       "0  [0.24253562503633297, 0.6063390625908325, 0.12...   \n",
       "\n",
       "                  video_features  \n",
       "0  [5877, 29, 202.6551724137931]  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#dataset to work with (so you can run this separately)\n",
    "full_dataset = pd.read_csv('./DATA/preprocessed_data.csv')\n",
    "full_dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ea716",
   "metadata": {},
   "source": [
    "## 4.1) Custom regression model (no pytorch just yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440530eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking inspiration from the gradient-descent models we did in PA4\n",
    "\n",
    "#first we have to flatten the columns\n",
    "\n",
    "#description and tags are weird since theyre not all 100\n",
    "description_vectors = full_dataset['description_vector'].apply(eval)\n",
    "description_vectors = description_vectors.apply(lambda x: x if len(x) == 100 else [0] * 100)\n",
    "description_vectors = np.array(description_vectors.tolist())\n",
    "tags_vectors = full_dataset['tags_vector'].apply(eval)\n",
    "tags_vectors = tags_vectors.apply(lambda x: x if len(x) == 100 else [0] * 100)\n",
    "tags_vectors = np.array(tags_vectors.tolist())\n",
    "\n",
    "\n",
    "video_features = np.array(full_dataset['video_features'].apply(eval).tolist())\n",
    "\n",
    "#setting all of X's features as the one-hots, description, tags, and video features (excl viewcount, likecount, and id)\n",
    "X = np.hstack([full_dataset.iloc[:, 3:-3].values, description_vectors, tags_vectors, video_features])\n",
    "y = full_dataset['view_count'].values #could be like_count aswell, but \"trending\" is views\n",
    "\n",
    "#actual regression model\n",
    "class CustomRegressionModel:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, scheduler=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.scheduler = scheduler\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def init_parameters(self, n_features):\n",
    "        \"\"\"Given the number of features, initialize weights\"\"\"\n",
    "        self.weights = np.random.randn(n_features) * 0.01 #small random values\n",
    "        self.bias = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Linear prediction of value X with weights and bias\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Given the h_theta and y, compute MSE\"\"\"\n",
    "        l2_penalty = 0.01 * np.sum(self.weights**2) #l2 regularization\n",
    "        return np.mean((y_true - y_pred)**2) + l2_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"Compute the gradients for the current set of weights\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        delta_weights = -(2 / n_samples) * np.dot(X.T, (y_true - y_pred)) + 0.02 * self.weights #l2 gradient\n",
    "        delta_bias    = -(2 / n_samples) * np.sum(y_true - y_pred)\n",
    "        return delta_weights, delta_bias\n",
    "    \n",
    "    def get_learning_vals(self):\n",
    "        \"\"\"Gets all learned values\"\"\"\n",
    "        return self.bias, self.weights\n",
    "\n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Learning/fitting function for the weights\"\"\"\n",
    "        self.init_parameters(X.shape[1])\n",
    "\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        patience = 100 #early stopping patience\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.predict(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "\n",
    "            #add losses pre-update to history\n",
    "            losses.append(loss)\n",
    "\n",
    "            #check if we have a new best loss\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            #check if we have a scheduler and run it if we do\n",
    "            if self.scheduler:\n",
    "                self.learning_rate = self.scheduler(epoch, self.learning_rate)\n",
    "\n",
    "            #calc gradients and run an update\n",
    "            dw, db = self.compute_gradients(X, y, y_pred)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias    -= self.learning_rate * db\n",
    "\n",
    "            if epoch % 50 == 0 and verbose:\n",
    "                print(f\"Epochs completed: {round((epoch/self.epochs) * 100, 2)}%\", end='\\r')\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epochs completed: 100.00%\")\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        #predict and compute MSE value for epoch-specific evaluation\n",
    "        y_pred = self.predict(X)\n",
    "        return self.compute_loss(y, y_pred)\n",
    "    \n",
    "#step decay scheduler, reducing the learning rate every 100 epochs\n",
    "def step_decay_scheduler(epoch, current_lr, drop=0.5, epochs_drop=100):\n",
    "    if epoch % epochs_drop == 0 and epoch > 0:\n",
    "        return current_lr * drop\n",
    "    return current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43044e0a",
   "metadata": {},
   "source": [
    "Before working with our actual data, let's sanity-check it with some synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fef58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4700\n",
      "Epochs completed: 100.00%\n",
      "Training MSE (Log-Transformed): 0.1795\n",
      "Testing MSE (Log-Transformed): 0.4154\n",
      "Testing MSE (Original Scale): 1465790789751329.5000\n",
      "Testing MAPE (Original Scale): 30.63%\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = {\n",
    "    \"cat\": [1, 0, 1, 0, 1],\n",
    "    \"dog\": [0, 1, 0, 1, 0],\n",
    "    \"table\": [1, 1, 0, 0, 1],\n",
    "    \"chair\": [0, 0, 1, 1, 0],\n",
    "    \"description_vector\": [[0.1] * 100] * 5,  #fake description vectors\n",
    "    \"tags_vector\": [[0.2] * 100] * 5,         #fake tags vectors\n",
    "    \"video_features\": [[300, 30, 10]] * 5,    #fake video features (frame count, fps, duration)\n",
    "    \"view_count\": [100000000, 50000000, 75000000, 25000000, 125000000]  #fake view counts\n",
    "}\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "synth_description_vectors = np.array(synthetic_df[\"description_vector\"].tolist())\n",
    "synth_tags_vectors = np.array(synthetic_df[\"tags_vector\"].tolist())\n",
    "synth_video_features = np.array(synthetic_df[\"video_features\"].tolist())\n",
    "X_synthetic = np.hstack([synthetic_df[[\"cat\", \"dog\", \"table\", \"chair\"]].values,\n",
    "                         synth_description_vectors,\n",
    "                         synth_tags_vectors,\n",
    "                         synth_video_features])\n",
    "y_synthetic = synthetic_df[\"view_count\"].values\n",
    "\n",
    "#normalize features and target variable\n",
    "X_mean_synth = np.mean(X_synthetic, axis=0)\n",
    "X_std_synth = np.std(X_synthetic, axis=0)\n",
    "X_std_synth[X_std_synth == 0] = 1\n",
    "X_synthetic = (X_synthetic - X_mean_synth) / X_std_synth\n",
    "\n",
    "y_mean_synth = np.mean(y_synthetic)\n",
    "y_std_synth = np.std(y_synthetic)\n",
    "y_synthetic = (y_synthetic - y_mean_synth) / y_std_synth\n",
    "\n",
    "split_idx = int(0.8 * len(X_synthetic))\n",
    "X_train_synthetic, X_test_synthetic = X_synthetic[:split_idx], X_synthetic[split_idx:]\n",
    "y_train_synthetic, y_test_synthetic = y_synthetic[:split_idx], y_synthetic[split_idx:]\n",
    "\n",
    "#apply log transform on target\n",
    "y_synthetic_log = np.log1p(synthetic_df[\"view_count\"].values)  # log1p(x) = log(1 + x)\n",
    "\n",
    "#normalize the log-transformed target variable\n",
    "y_mean_log = np.mean(y_synthetic_log)\n",
    "y_std_log = np.std(y_synthetic_log)\n",
    "y_synthetic_log = (y_synthetic_log - y_mean_log) / y_std_log\n",
    "\n",
    "#split into training and testing sets\n",
    "y_train_synthetic_log, y_test_synthetic_log = y_synthetic_log[:split_idx], y_synthetic_log[split_idx:]\n",
    "\n",
    "## Actually training the model here:\n",
    "\n",
    "model = CustomRegressionModel(learning_rate=0.001, epochs=10000, scheduler=step_decay_scheduler)\n",
    "losses = model.fit(X_train_synthetic, y_train_synthetic_log)\n",
    "\n",
    "#evaluate the model\n",
    "train_mse = model.evaluate(X_train_synthetic, y_train_synthetic_log)\n",
    "test_mse = model.evaluate(X_test_synthetic, y_test_synthetic_log)\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_log = model.predict(X_test_synthetic)\n",
    "y_pred_original = np.expm1((y_pred_log * y_std_log) + y_mean_log)\n",
    "y_test_original = np.expm1((y_test_synthetic_log * y_std_log) + y_mean_log)\n",
    "\n",
    "#compute MSE on the original scale\n",
    "mse_original_scale = np.mean((y_test_original - y_pred_original) ** 2)\n",
    "\n",
    "#display results\n",
    "print(f\"Training MSE (Log-Transformed): {train_mse:.4f}\")\n",
    "print(f\"Testing MSE (Log-Transformed): {test_mse:.4f}\")\n",
    "print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "\n",
    "#instead of MSE, we could possibly use MAPE (Mean Absolute Percentage Error) for a more interpretable metric\n",
    "mape_original_scale = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90b516",
   "metadata": {},
   "source": [
    "Now that the results look good (relatively) in regards to MAPE lets actually run it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7857d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##doing pretty much the same thing as above, but with the real data\n",
    "#normalize the features\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "std[std == 0] = 1\n",
    "X = (X - mean) / std\n",
    "\n",
    "y_log = np.log1p(y)  # log1p(x) = log(1 + x)\n",
    "\n",
    "#normalize the log-transformed target variable\n",
    "y_mean = np.mean(y_log)\n",
    "y_std = np.std(y_log)\n",
    "y_log = (y_log - y_mean) / y_std\n",
    "\n",
    "#split into training/testing 80/20\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_log_train, y_log_test = y_log[:split_idx], y_log[split_idx:]\n",
    "\n",
    "#check real quick if any nan's exist anywhere\n",
    "if np.isnan(X).any():\n",
    "    print(\"Warning: NaN values detected in the feature matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed scanning all learning rates and decayers.\n",
      "Top 5 Results:\n",
      "Learning Rate: 0.0003527236180904523, Decayer: False, MSE: 3110512620577.4990, MAPE: 158.53%\n",
      "Learning Rate: 0.0004029698492462312, Decayer: False, MSE: 3208166523954.9541, MAPE: 156.84%\n",
      "Learning Rate: 0.00030247738693467343, Decayer: False, MSE: 3267566598310.0776, MAPE: 161.38%\n",
      "Learning Rate: 0.008995075376884421, Decayer: True, MSE: 3282245526457.7515, MAPE: 152.07%\n",
      "Learning Rate: 0.000503462311557789, Decayer: False, MSE: 3321476432493.0234, MAPE: 161.21%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the best learning rate of 0.0003527236180904523 and decayer of False, we tested again and found: \n",
      "MSE of 3697471070977.4360 and MAPE of 167.90%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_custom_linear_regression(learning_rate, plot=False, decayer=False):\n",
    "    epchs = 10000\n",
    "\n",
    "    #first we train it on our data (using decayer if we want to)\n",
    "    model = CustomRegressionModel(learning_rate=learning_rate, epochs=epchs, scheduler=step_decay_scheduler) if decayer else CustomRegressionModel(learning_rate=learning_rate, epochs=epchs)\n",
    "    losses = model.fit(X_train, y_log_train, verbose=False)\n",
    "\n",
    "    #then we plot it if true\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(losses)), losses, label=\"Training Loss\", color=\"blue\")\n",
    "        plt.title(\"Loss Over Time\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #show the MSE results\n",
    "    train_mse = model.evaluate(X_train, y_log_train) #strictly MSE in the log space, not directly used\n",
    "    test_mse = model.evaluate(X_test, y_log_test) #strictly MSE in the log space, not directly used\n",
    "\n",
    "    #reverse the log transformation for predictions\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred_original = np.expm1((y_pred_log * y_std) + y_mean)\n",
    "    y_test_original = np.expm1((y_log_test * y_std) + y_mean)\n",
    "\n",
    "    #compute MSE on the original scale\n",
    "    mse_original_scale = np.mean((y_test_original - y_pred_original) ** 2)\n",
    "\n",
    "    #instead of MSE, we use MAPE\n",
    "    mape_original_scale = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "\n",
    "    return mse_original_scale, mape_original_scale\n",
    "\n",
    "#testing the model with a search space of learning rates (200 total values) and decayer or not (2 values, yes or no), total of 400 values\n",
    "learning_rates = np.linspace(0.000001, 0.01, 200)\n",
    "decayer = [True, False]\n",
    "results = []\n",
    "for lr in learning_rates:\n",
    "    for d in decayer:\n",
    "        print(f\"Now on: learning Rate: {lr:.6f}, Decayer: {str(d)[0]}\", end=\"\\r\")\n",
    "\n",
    "        mse, mape = test_custom_linear_regression(lr, plot=False, decayer=d)\n",
    "        results.append((lr, d, mse, mape))\n",
    "        #print(f\"Learning Rate: {lr}, Decayer: {d}, MSE: {mse:.4f}, MAPE: {mape:.2f}%\")\n",
    "\n",
    "print(\"Completed scanning all learning rates and decayers.\")\n",
    "\n",
    "#sort results by MSE, print top 5\n",
    "sorted_results = sorted(results, key=lambda x: x[2])\n",
    "print(\"Top 5 Results:\")\n",
    "for lr, d, mse, mape in sorted_results[:5]:\n",
    "    print(f\"Learning Rate: {lr}, Decayer: {d}, MSE: {mse:.4f}, MAPE: {mape:.2f}%\")\n",
    "\n",
    "#plot the best result\n",
    "best_result = sorted_results[0]\n",
    "best_lr, best_decayer, best_found_mse, best_found_mape = best_result\n",
    "experimented_mse, experimented_mape = test_custom_linear_regression(best_lr, plot=True, decayer=best_decayer)\n",
    "print(f\"Given the best learning rate of {best_lr} and decayer of {best_decayer}, we tested again and found: \\nMSE of {experimented_mse:.4f} and MAPE of {experimented_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f0a19",
   "metadata": {},
   "source": [
    "Looking at those top 5 values, we can clearly see a trend of the best learning rates and if a decayer is actually useful based on the MAPE value, and we can see the raw MSE in the original scale and MAPE values in that last returned line. With this, we can see that it is difficult to get an exact linear regression since we have a bunch of features, and not all features have the same \"weight\"/importance that the \"one-hot\"-equivalent features have.\n",
    "\n",
    "### Result:\n",
    "A `step_decay_scheduler`, as we previously defined, seems to somewhat impact the result, however, it seems as though our lowest MSE (in the original space) is around `3,697,471,070,977.4360`, with the MAPE being around ~`170`%. And, since the initial values of the weights are generated randomly, even though we chose the best searched value, we didn't get exactly the same result as before. This means that technically, if we were to randomize it further, we could do better, however, it seems as though we are hitting a low limit with just a normal regression model.\n",
    "\n",
    "Technically, we could add more features or control it further by specifying what weights should be emphasized, but treating them as all \"the same\", even with standardized/logged features, we still get a large resulting MSE and MAPE value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5421f6",
   "metadata": {},
   "source": [
    "## 4.2) Baseline sklearn regression model (scoring by mean-squared-error) Our end goal is to at least get a better score than this.\n",
    "\n",
    "Now that we have our custom regression model, let's see if it actually performs better than the sklearn regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83709f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100.00%\n",
      "Sklearn Linear Regression - MSE (Original Scale): 16853464201954518.0000\n",
      "Sklearn Linear Regression - MAPE (Original Scale): 361.33%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor #using stochastic gradient descent for sklearn's linear regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "#for ignoring any warnings from skLearn (we only do this because we want to map the losses to the epochs)\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "#initialize sklearn's SGD Regression model (loss=\"squared_error\" is the default, but just to be explicit)\n",
    "#all parameters here are attempting to match the custom model as closely as possible\n",
    "sklearn_model = SGDRegressor(loss=\"squared_error\",\n",
    "                             learning_rate='constant',\n",
    "                             max_iter=1,\n",
    "                             warm_start=True,\n",
    "                             eta0=0.0003527236180904523, #learning rate (0.0003527236180904523 is the best learning rate found from the custom model)\n",
    "                             alpha=0.01) #l2 regularization\n",
    "\n",
    "losses = []\n",
    "\n",
    "#same epoch range as before, so we can directly compare the two @10k epochs\n",
    "for epoch in range(10000):\n",
    "    sklearn_model.fit(X_train, y_log_train)\n",
    "    y_log_pred_train = sklearn_model.predict(X_train)\n",
    "    loss = mean_squared_error(y_log_train, y_log_pred_train)\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f\"Epochs completed: {round((epoch/10000) * 100, 2)}%\", end='\\r')\n",
    "print(f\"Epochs completed: 100.00%\")\n",
    "\n",
    "#predict on the test set\n",
    "y_log_pred_sklearn = sklearn_model.predict(X_test)\n",
    "y_log_pred_sklearn = np.clip(y_log_pred_sklearn, -1e2, 1e2) #clip value further to avoid runtime overflows\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_original_sklearn = np.expm1((y_log_pred_sklearn * y_std) + y_mean)\n",
    "y_test_original_sklearn = np.expm1((y_log_test * y_std) + y_mean)\n",
    "\n",
    "#clip both values to 0 and limit infinity (since we can't have negative views)\n",
    "y_pred_original_sklearn = np.clip(y_pred_original_sklearn, 0, 1e10)\n",
    "y_test_original_sklearn = np.clip(y_test_original_sklearn, 0, 1e10)\n",
    "\n",
    "#compute MSE and MAPE on the original scale\n",
    "mse_sklearn = mean_squared_error(y_test_original_sklearn, y_pred_original_sklearn)\n",
    "mape_sklearn = mean_absolute_percentage_error(y_test_original_sklearn, y_pred_original_sklearn) * 100\n",
    "\n",
    "#print the results\n",
    "print(f\"Sklearn Linear Regression - MSE (Original Scale): {mse_sklearn:.4f}\")\n",
    "print(f\"Sklearn Linear Regression - MAPE (Original Scale): {mape_sklearn:.2f}%\")\n",
    "\n",
    "#plot the losses over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(losses)), losses, label=\"Training Loss\", color=\"blue\")\n",
    "plt.title(\"Loss Over Time (SGD Regressor)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05d68",
   "metadata": {},
   "source": [
    "Again, the MSE displayed in the graph is shown in the log scale, so we could see that on average, it's hovering around the 0.4 mark. However, when looking at the MSE and MAPE in the original scales, we can clearly see the comparisons between the custom and the sklearn models:\n",
    "\n",
    "Custom MSE: 3697471070977.4360\n",
    "\n",
    "SkLearn MSE: 16853464201954518.0000\n",
    "\n",
    "Custom MAPE: 167.90%\n",
    "\n",
    "SkLearn MAPE: 361.33%\n",
    "<br>\n",
    "<br>\n",
    "Just comparing the Custom regression model versus SkLearn's model (that is customized to be as close as possible to ours, and has been tested strenuously in the past with different epoch and configuration values), our regression model tends to perform better in terms of MSE and MAPE.\n",
    "\n",
    "Now, to enforce my point, I will perform hyperparameter search on the SkLearn's SGD model and show the best MSE and MAPE values!\n",
    "\n",
    "<strong>PLEASE KEEP IN MIND, THE FOLLOWING CODE TESTS ~1.6 MILLION POSSIBLE COMBINATIONS OF FEATURES, IT IS REPEATABLE WITH THE RANDOM_STATE OF 42, SO I WOULDNT RECOMMEND YOU RUNNING IT UNLESS YOU WANT TO TAKE UP YOUR CPU FOR HALF AN HOUR OR LONGER</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38582276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:Loss: squared_epsilon_insensitive | LR of: adaptive | Eta of: 0.010000\n",
      "Loss: epsilon_insensitive, Learning Rate: constant, Eta0: 0.010000, Alpha: 0.08889000000000001, Max Iter: 1000, Epsilon: 0.46, Penalty: l2, L1 Ratio: 0.00, MSE: 2221794936689.0044, MAPE: 135.80%\n",
      "Loss: epsilon_insensitive, Learning Rate: constant, Eta0: 0.010000, Alpha: 0.08889000000000001, Max Iter: 3250, Epsilon: 0.46, Penalty: l2, L1 Ratio: 0.00, MSE: 2221794936689.0044, MAPE: 135.80%\n",
      "Loss: epsilon_insensitive, Learning Rate: constant, Eta0: 0.010000, Alpha: 0.08889000000000001, Max Iter: 5500, Epsilon: 0.46, Penalty: l2, L1 Ratio: 0.00, MSE: 2221794936689.0044, MAPE: 135.80%\n",
      "Loss: epsilon_insensitive, Learning Rate: constant, Eta0: 0.010000, Alpha: 0.08889000000000001, Max Iter: 7750, Epsilon: 0.46, Penalty: l2, L1 Ratio: 0.00, MSE: 2221794936689.0044, MAPE: 135.80%\n",
      "Loss: epsilon_insensitive, Learning Rate: constant, Eta0: 0.010000, Alpha: 0.08889000000000001, Max Iter: 10000, Epsilon: 0.46, Penalty: l2, L1 Ratio: 0.00, MSE: 2221794936689.0044, MAPE: 135.80%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "#for ignoring any warnings from skLearn (we only do this because we want to map the losses to the epochs)\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "#defining hyperparameter search space\n",
    "loss_functions = ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "learning_rates = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "penalties = ['l2', 'l1', 'elasticnet', None]  #regularization penalties\n",
    "l1_ratios = np.linspace(0, 1, 5)  #l1 ratio for elasticnet\n",
    "etas = np.linspace(0.00001, 0.01, 10)  #learning rate values\n",
    "alphas = np.linspace(1e-5, 0.1, 10)  #regularization strength\n",
    "max_iters = np.linspace(1000, 10000, 5)  #number of iterations\n",
    "epsilons = np.linspace(0.1, 0.5, 10)  #epsilon values for epsilon-insensitive loss\n",
    "\n",
    "results = []\n",
    "\n",
    "#perform grid search\n",
    "for loss in loss_functions:\n",
    "    for learning_rate in learning_rates:\n",
    "        for eta in etas:\n",
    "            print(f\"Currently on: Loss: {loss} | LR of: {learning_rate} | Eta of: {eta:.4f}\", end=\"\\r\")\n",
    "            for alpha in alphas:\n",
    "                for epsilon in epsilons:\n",
    "                    for penalty in penalties:\n",
    "                        for l1_ratio in l1_ratios:\n",
    "                            for max_iter in max_iters:\n",
    "                                if loss == 'epsilon_insensitive' and learning_rate != 'constant':\n",
    "                                    #epsilon-insensitive loss only works with constant learning rate\n",
    "                                    continue\n",
    "\n",
    "                                #convert iterations to int\n",
    "                                max_iter = int(max_iter)\n",
    "\n",
    "                                #initialize the model with current hyperparameters\n",
    "                                model = SGDRegressor(\n",
    "                                    loss=loss,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    eta0=eta,\n",
    "                                    alpha=alpha,\n",
    "                                    max_iter=max_iter,\n",
    "                                    epsilon=epsilon,\n",
    "                                    penalty=penalty,\n",
    "                                    l1_ratio=l1_ratio,\n",
    "                                    random_state=42, #set a random state for reproducibility\n",
    "                                    warm_start=False\n",
    "                                )\n",
    "                                \n",
    "                                #train the model\n",
    "                                model.fit(X_train, y_log_train)\n",
    "                                \n",
    "                                #predict on the test set\n",
    "                                y_log_pred = model.predict(X_test)\n",
    "                                y_log_pred = np.clip(y_log_pred, -1e2, 1e2) #clip values to avoid overflow\n",
    "                                \n",
    "                                #reverse the log transformation for predictions\n",
    "                                y_pred_original = np.expm1((y_log_pred * y_std) + y_mean)\n",
    "                                y_test_original = np.expm1((y_log_test * y_std) + y_mean)\n",
    "                                \n",
    "                                #clip values to avoid invalid numbers\n",
    "                                y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "                                y_test_original = np.clip(y_test_original, 0, 1e10)\n",
    "                                \n",
    "                                #compute MSE and MAPE\n",
    "                                mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "                                mape = mean_absolute_percentage_error(y_test_original, y_pred_original) * 100\n",
    "                                \n",
    "                                #store the results\n",
    "                                results.append({\n",
    "                                    'loss': loss,\n",
    "                                    'learning_rate': learning_rate,\n",
    "                                    'eta0': eta,\n",
    "                                    'alpha': alpha,\n",
    "                                    'max_iter': max_iter,\n",
    "                                    'epsilon': epsilon,\n",
    "                                    'penalty': penalty,\n",
    "                                    'l1_ratio': l1_ratio,\n",
    "                                    'mse': mse,\n",
    "                                    'mape': mape\n",
    "                                })\n",
    "\n",
    "                        if penalty != 'elasticnet':\n",
    "                            #l1 ratio is only used for elasticnet penalty (sk documentation said not)\n",
    "                            break\n",
    "\n",
    "                    if loss == 'squared_error':\n",
    "                        #squared error doesn't need to be tested with different epsilons (sk documentation said not)\n",
    "                        break\n",
    "\n",
    "            if learning_rate == 'optimal':\n",
    "                #optimal learning rate doesn't need to be tested with different etas (sk documentation said not)\n",
    "                break\n",
    "\n",
    "#sort results by MAPE\n",
    "sorted_results = sorted(results, key=lambda x: x['mse'])\n",
    "\n",
    "#print the top 5 results\n",
    "print(\"Top 5 Results:\")\n",
    "for res in sorted_results[:5]:\n",
    "    print(f\"Loss: {res['loss']}, Learning Rate: {res['learning_rate']}, \"\n",
    "          f\"Eta0: {res['eta0']:.6f}, Alpha: {res['alpha']}, Max Iter: {res['max_iter']}, \"\n",
    "          f\"Epsilon: {res['epsilon']:.2f}, Penalty: {res['penalty']}, L1 Ratio: {res['l1_ratio']:.2f}, \"\n",
    "          f\"MSE: {res['mse']:.4f}, MAPE: {res['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564a891",
   "metadata": {},
   "source": [
    "In these examples, we are again sorting by MSE, and we can see that with the SkLearn parameter optimization (and doing things like keeping the learning rate constant and having iterations around 1,000), we do end up getting better results, comparing our MSE's of `2,221,794,936,689.0044` (SK) to `3,697,471,070,977.4360`(base custom). \n",
    "\n",
    "### First conclusion about this mini section:\n",
    "Although with parameter optimization we ended up getting a better result than our Custom model, with the same general concepts and hyperparameters applied, we ended up performing better with our custom model by a sizeable factor, meaning that we got some form of success with our constants! Not necessarily better than the \"best of the best\" SGD parameters, but still something!\n",
    "\n",
    "To dig a bit further, let's try and see if we could alter our model to use optimal SkLearn parameters: the epsilon_insensitive loss function, a constant learning rate of 0.01, alpha of 0.08889000000000001, and max iterations of 1000, epsilon of 0.46, and only l2 penalty and see if we can get some improvement using an altered custom model vs the sklearn's optimized model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2cb8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRegressionModel:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, epsilon=0.46, alpha=0.08889):\n",
    "        self.learning_rate = learning_rate #eta0=0.01\n",
    "        self.epochs = epochs #max_iter=1000\n",
    "        self.epsilon = epsilon  #epsilon=0.46\n",
    "        self.alpha = alpha  #alpha=0.08889 (l2 reg strength)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def init_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias.\"\"\"\n",
    "        self.weights = np.random.randn(n_features) * 0.01 \n",
    "        self.bias = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Linear prediction.\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute epsilon-insensitive loss with L2 regularization.\"\"\"\n",
    "        errors = np.abs(y_true - y_pred) - self.epsilon\n",
    "        errors = np.maximum(errors, 0)  #apply epsilon-insensitive threshold\n",
    "\n",
    "        l2_penalty = self.alpha * np.sum(self.weights**2)  #l2 regularization (same as before, but now parameterized alpha)\n",
    "        return np.mean(errors**2) + l2_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"Compute gradients for weights and bias.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        residuals = y_true - y_pred\n",
    "        mask = np.abs(residuals) > self.epsilon  #only update for residuals > epsilon\n",
    "\n",
    "        delta_weights = -(2 / n_samples) * np.dot(X[mask].T, residuals[mask]) + 2 * self.alpha * self.weights\n",
    "        delta_bias    = -(2 / n_samples) * np.sum(residuals[mask])\n",
    "        \n",
    "        return delta_weights, delta_bias\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the model using gradient descent.\"\"\"\n",
    "        self.init_parameters(X.shape[1])\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.predict(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "\n",
    "            #compute gradients and update weights and bias\n",
    "            dw, db = self.compute_gradients(X, y, y_pred)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias    -= self.learning_rate * db\n",
    "\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate the model on a dataset.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return self.compute_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4928df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.3900\n",
      "Epoch 100/1000, Loss: 0.1500\n",
      "Epoch 200/1000, Loss: 0.1419\n",
      "Epoch 300/1000, Loss: 0.1407\n",
      "Epoch 400/1000, Loss: 0.1407\n",
      "Epoch 500/1000, Loss: 0.1411\n",
      "Epoch 600/1000, Loss: 0.1415\n",
      "Epoch 700/1000, Loss: 0.1432\n",
      "Epoch 800/1000, Loss: 0.1423\n",
      "Epoch 900/1000, Loss: 0.1423\n",
      "Training MSE (Log-Transformed): 0.1427\n",
      "Testing MSE (Log-Transformed): 0.7716\n",
      "Testing MSE (Original Scale): 2423766847630.3486\n",
      "Testing MAPE (Original Scale): 141.32%\n"
     ]
    }
   ],
   "source": [
    "#now that we have that class definition, let's actually train it and see how it does!\n",
    "model = OptimizedRegressionModel(learning_rate=0.01, epochs=1000, epsilon=0.46, alpha=0.08889)\n",
    "\n",
    "#fit\n",
    "losses = model.fit(X_train, y_log_train, verbose=True)\n",
    "\n",
    "#eval the model\n",
    "train_mse = model.evaluate(X_train, y_log_train)\n",
    "test_mse = model.evaluate(X_test, y_log_test)\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_log = model.predict(X_test)\n",
    "y_pred_original = np.expm1((y_pred_log * y_std) + y_mean)\n",
    "y_test_original = np.expm1((y_log_test * y_std) + y_mean)\n",
    "\n",
    "#clip both values to 0 and limit infinity (since we can't have negative views)\n",
    "y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "y_test_original = np.clip(y_test_original, 0, 1e10)\n",
    "\n",
    "#compute MSE on the original scale\n",
    "mse_original_scale = np.mean((y_test_original - y_pred_original) ** 2)\n",
    "\n",
    "#compute MAPE on the original scale\n",
    "mape_original_scale = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "\n",
    "print(f\"Training MSE (Log-Transformed): {train_mse:.4f}\")\n",
    "print(f\"Testing MSE (Log-Transformed): {test_mse:.4f}\")\n",
    "print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e433320",
   "metadata": {},
   "source": [
    "With that, we can see that our updated MSE on the original scale is `2,423,766,847,630.3486`, which is definitely an improvement from the original parameters (with an improvement of ~600 billion MSE), but it is still lacking in comparison to the ~200 billion MSE further improvement.\n",
    "\n",
    "### Second conclusion about this mini section:\n",
    "\n",
    "Although we did perform better with our naive estimates in comparison to SkLearn, and after optimizing more hyperparameters we got a better result and ended up having a higher MSE than SkLearn, our average MSE are still in the trillions, which is absolutely not good in regards to youtube video viewcounts. It's a better analysis than just guessing random numbers, but we couldn't get an accurate enough prediction to extrapolate any data from it. Maybe, it's not as simple as \"cat+dog=100M views\", maybe something outside of a regression model, something more like a neural network, might perform better with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbec7b4",
   "metadata": {},
   "source": [
    "### 4.2.5) Large tangent, feature representation\n",
    "\n",
    "With these results, I had the idea that maybe the actual data features were too much, as it might take longer for a system to finalize it's results (or optimal feature weights), so in this subsection, I will go about with only having two X features per datapoint, the \"thumbnail contents\" and \"video features\", as those are the primary things the user would see before actually clicking/viewing the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a2bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the \"thumbnail images\" should be a singular array with the encoded values\n",
    "#the \"video features\" should be a singular array with the encoded values of the video features (frame count, fps, duration)\n",
    "combined_thumbnail_features = full_dataset.iloc[:, 3:-3].values\n",
    "combined_video_features = np.array(full_dataset['video_features'].apply(eval).tolist()) #last column is video features\n",
    "\n",
    "#we're not considering the description or tags, as the user would not see those in the decision window\n",
    "\n",
    "X_simplified = np.hstack([combined_thumbnail_features, combined_video_features])\n",
    "y_simplified = full_dataset['view_count'].values #same as before, prediction metric is the view count\n",
    "\n",
    "#normalize the features and target variable\n",
    "X_mean_simplified = np.mean(X_simplified, axis=0)\n",
    "X_std_simplified = np.std(X_simplified, axis=0)\n",
    "X_std_simplified[X_std_simplified == 0] = 1\n",
    "X_simplified = (X_simplified - X_mean_simplified) / X_std_simplified\n",
    "\n",
    "#y put into logspace\n",
    "y_simplified_log = np.log1p(y_simplified)  # log1p(x) = log(1 + x)\n",
    "#normalize the log-transformed target variable\n",
    "y_mean_simplified = np.mean(y_simplified_log)\n",
    "y_std_simplified = np.std(y_simplified_log)\n",
    "y_simplified_log = (y_simplified_log - y_mean_simplified) / y_std_simplified\n",
    "\n",
    "#split into training/testing 80/20\n",
    "split_idx = int(0.8 * len(X_simplified))\n",
    "\n",
    "X_train_simplified, X_test_simplified = X_simplified[:split_idx], X_simplified[split_idx:]\n",
    "y_train_simplified_log, y_test_simplified_log = y_simplified_log[:split_idx], y_simplified_log[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8cc27",
   "metadata": {},
   "source": [
    "Now, let's get a comparison of using the three models on the simplistic data, and log any improvements or losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54bd3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100.00%\n",
      "Base Model Training MSE (Log-Transformed): 0.4397\n",
      "Base Model Testing MSE (Log-Transformed): 1.4930\n",
      "Base Model Testing MSE (Original Scale): 2645115997334.0781\n",
      "Base Model Testing MAPE (Original Scale): 129.13%\n"
     ]
    }
   ],
   "source": [
    "#the base custom regression model (with optimally found hyperparameters)\n",
    "base_model = CustomRegressionModel(learning_rate=0.0003527236180904523, epochs=10000)\n",
    "base_model.fit(X_train_simplified, y_train_simplified_log, verbose=True)\n",
    "\n",
    "#evaluate the model\n",
    "train_mse_base = base_model.evaluate(X_train_simplified, y_train_simplified_log)\n",
    "test_mse_base = base_model.evaluate(X_test_simplified, y_test_simplified_log)\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_log_base = base_model.predict(X_test_simplified)\n",
    "y_pred_original_base = np.expm1((y_pred_log_base * y_std_simplified) + y_mean_simplified)\n",
    "y_test_original_base = np.expm1((y_test_simplified_log * y_std_simplified) + y_mean_simplified)\n",
    "\n",
    "#clip both values to 0 and limit infinity (since we can't have negative views)\n",
    "y_pred_original_base = np.clip(y_pred_original_base, 0, 1e10)\n",
    "y_test_original_base = np.clip(y_test_original_base, 0, 1e10)\n",
    "\n",
    "#compute MSE on the original scale\n",
    "mse_original_scale_base = np.mean((y_test_original_base - y_pred_original_base) ** 2)\n",
    "\n",
    "#compute MAPE on the original scale\n",
    "mape_original_scale_base = np.mean(np.abs((y_test_original_base - y_pred_original_base) / y_test_original_base)) * 100\n",
    "\n",
    "print(f\"Base Model Training MSE (Log-Transformed): {train_mse_base:.4f}\")\n",
    "print(f\"Base Model Testing MSE (Log-Transformed): {test_mse_base:.4f}\")\n",
    "print(f\"Base Model Testing MSE (Original Scale): {mse_original_scale_base:.4f}\")\n",
    "print(f\"Base Model Testing MAPE (Original Scale): {mape_original_scale_base:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d476680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Model - MSE (Original Scale): 2721968882395.4146\n",
      "Sklearn Model - MAPE (Original Scale): 108.45%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "#the sklearn model (with optimal hyperparameters)\n",
    "sklearn_model = SGDRegressor(loss='epsilon_insensitive', learning_rate='constant', eta0=0.01, alpha=0.08889, max_iter=1000, epsilon=0.46, penalty='l2', random_state=42)\n",
    "\n",
    "sklearn_model.fit(X_train_simplified, y_train_simplified_log)\n",
    "\n",
    "#predict on the test set\n",
    "y_log_pred_sklearn = sklearn_model.predict(X_test_simplified)\n",
    "y_log_pred_sklearn = np.clip(y_log_pred_sklearn, -1e2, 1e2) #clip value further to avoid runtime overflows\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_original_sklearn = np.expm1((y_log_pred_sklearn * y_std_simplified) + y_mean_simplified)\n",
    "y_test_original_sklearn = np.expm1((y_test_simplified_log * y_std_simplified) + y_mean_simplified)\n",
    "\n",
    "#clip both values to 0 and limit infinity (since we can't have negative views)\n",
    "y_pred_original_sklearn = np.clip(y_pred_original_sklearn, 0, 1e10)\n",
    "y_test_original_sklearn = np.clip(y_test_original_sklearn, 0, 1e10)\n",
    "\n",
    "#compute MSE and MAPE on the original scale\n",
    "mse_sklearn = mean_squared_error(y_test_original_sklearn, y_pred_original_sklearn)\n",
    "mape_sklearn = mean_absolute_percentage_error(y_test_original_sklearn, y_pred_original_sklearn) * 100\n",
    "\n",
    "print(f\"Sklearn Model - MSE (Original Scale): {mse_sklearn:.4f}\")\n",
    "print(f\"Sklearn Model - MAPE (Original Scale): {mape_sklearn:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a12cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.3859\n",
      "Epoch 100/1000, Loss: 0.1894\n",
      "Epoch 200/1000, Loss: 0.1823\n",
      "Epoch 300/1000, Loss: 0.1809\n",
      "Epoch 400/1000, Loss: 0.1808\n",
      "Epoch 500/1000, Loss: 0.1808\n",
      "Epoch 600/1000, Loss: 0.1809\n",
      "Epoch 700/1000, Loss: 0.1811\n",
      "Epoch 800/1000, Loss: 0.1813\n",
      "Epoch 900/1000, Loss: 0.1814\n",
      "Optimized Model Training MSE (Log-Transformed): 0.1815\n",
      "Optimized Model Testing MSE (Log-Transformed): 0.8698\n",
      "Optimized Model Testing MSE (Original Scale): 2647055368493.3208\n",
      "Optimized Model Testing MAPE (Original Scale): 135.42%\n"
     ]
    }
   ],
   "source": [
    "#the optimized regression model (with optimal hyperparameters)\n",
    "optimized_model = OptimizedRegressionModel(learning_rate=0.01, epochs=1000, epsilon=0.46, alpha=0.08889)\n",
    "optimized_model.fit(X_train_simplified, y_train_simplified_log, verbose=True)\n",
    "\n",
    "#evaluate the model\n",
    "train_mse_optimized = optimized_model.evaluate(X_train_simplified, y_train_simplified_log)\n",
    "test_mse_optimized = optimized_model.evaluate(X_test_simplified, y_test_simplified_log)\n",
    "\n",
    "#reverse the log transformation for predictions\n",
    "y_pred_log_optimized = optimized_model.predict(X_test_simplified)\n",
    "y_pred_original_optimized = np.expm1((y_pred_log_optimized * y_std_simplified) + y_mean_simplified)\n",
    "y_test_original_optimized = np.expm1((y_test_simplified_log * y_std_simplified) + y_mean_simplified)\n",
    "\n",
    "#clip both values to 0 and limit infinity (since we can't have negative views)\n",
    "y_pred_original_optimized = np.clip(y_pred_original_optimized, 0, 1e10)\n",
    "y_test_original_optimized = np.clip(y_test_original_optimized, 0, 1e10)\n",
    "\n",
    "#compute MSE on the original scale\n",
    "mse_original_scale_optimized = np.mean((y_test_original_optimized - y_pred_original_optimized) ** 2)\n",
    "\n",
    "#compute MAPE on the original scale\n",
    "mape_original_scale_optimized = np.mean(np.abs((y_test_original_optimized - y_pred_original_optimized) / y_test_original_optimized)) * 100\n",
    "\n",
    "print(f\"Optimized Model Training MSE (Log-Transformed): {train_mse_optimized:.4f}\")\n",
    "print(f\"Optimized Model Testing MSE (Log-Transformed): {test_mse_optimized:.4f}\")\n",
    "print(f\"Optimized Model Testing MSE (Original Scale): {mse_original_scale_optimized:.4f}\")\n",
    "print(f\"Optimized Model Testing MAPE (Original Scale): {mape_original_scale_optimized:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bd4c5",
   "metadata": {},
   "source": [
    "Comparing the results from the expanded vs simplified data gives us the following (only using MAPE scores, as thats a better direct comparison between models):\n",
    "\n",
    "1. Base custom model with optimal hyperparameters: $167.90\\% \\rightarrow 129.13\\%$\n",
    "2. SkLearn model with optimal hyperparameters: $135.80\\% \\rightarrow 108.45\\%$\n",
    "3. Optimized custom model: $141.32\\% \\rightarrow 135.42\\%$\n",
    "\n",
    "In some cases more than others (possibly likely due to randomization playing a role) there was a decrease in the MAPE results, thus meaning that as we simplified the data (and didn't utilize features like \"tags\" and \"descriptions\"), we got a higher MAPE accuracy. It's still in ridiculous figures, as it is still in the trillions, but if there was an improvement on any part, thats good enough for this tangent!\n",
    "\n",
    "<strong>Now, let's actually go on to working with neural networks and seeing if we can get an improvement!</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0c70b",
   "metadata": {},
   "source": [
    "## 4.3) Pytorch neural network model with ADAM optimization\n",
    "\n",
    "For this, let's imagine that regression isn't exactly the way to go. This means that we have to move from normal SkLearn and attempt to use the \"state of the art\" for commonly available neural networks (that aren't transformer models) to see if we can get a better result than all regression models (in regards to MSE and MAPE scores)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fad25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class BaseCustomNNModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BaseCustomNNModel, self).__init__()\n",
    "\n",
    "        #input -> 128 -> 64 -> 1 (output)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#function to measure the loss of the model\n",
    "def train_model(model, optimizer, criterion, train_loader, epochs, verbose=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        if verbose and (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\", end='\\r')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Epoch {epochs}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "#function to evaluate the model\n",
    "def evaluate_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            y_true.extend(y_batch.tolist())\n",
    "            y_pred.extend(predictions.tolist())\n",
    "    return total_loss / len(test_loader), y_true, y_pred\n",
    "\n",
    "#prepare the data for pytorch itself\n",
    "def prepare_data(X, y, batch_size=32):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "#function to run the experiment with the pytorch model\n",
    "def run_exp(X_train, y_train, X_test, y_test, input_size, epochs, batch_size=32):\n",
    "    #prepare data loaders\n",
    "    train_loader = prepare_data(X_train, y_train, batch_size)\n",
    "    test_loader = prepare_data(X_test, y_test, batch_size)\n",
    "\n",
    "    #initialize the model, optimizer, and loss function\n",
    "    model = BaseCustomNNModel(input_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  #using ADAM optimizer\n",
    "    criterion = nn.MSELoss() #focusing on MSE loss, can transfer over to MAPE later\n",
    "\n",
    "    #train the model\n",
    "    print(\"Training the model...\")\n",
    "    train_model(model, optimizer, criterion, train_loader, epochs)\n",
    "\n",
    "    #evaluate the model\n",
    "    print(\"Evaluating the model...\")\n",
    "    test_loss, y_true, y_pred = evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "    #reverse log transformation for predictions\n",
    "    y_pred_original = np.expm1((np.array(y_pred) * y_std) + y_mean)\n",
    "    y_true_original = np.expm1((np.array(y_true) * y_std) + y_mean)\n",
    "\n",
    "    #clip values to avoid invalid numbers\n",
    "    y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "    y_true_original = np.clip(y_true_original, 0, 1e10)\n",
    "\n",
    "    #compute MSE and MAPE on the original scale\n",
    "    mse_original_scale = np.mean((y_true_original - y_pred_original) ** 2)\n",
    "    mape_original_scale = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100\n",
    "\n",
    "    print(f\"Test Loss (Log-Transformed): {test_loss:.4f}\")\n",
    "    print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "    print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c2445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on normal X data...\n",
      "Training the model...\n",
      "Epoch 10000/10000, Loss: 0.0079\n",
      "Evaluating the model...\n",
      "Test Loss (Log-Transformed): 1.6364\n",
      "Testing MSE (Original Scale): 53567153436861.9609\n",
      "Testing MAPE (Original Scale): 310.11%\n",
      "\n",
      "Running experiment on simplified X data...\n",
      "Training the model...\n",
      "Epoch 10000/10000, Loss: 0.0215\n",
      "Evaluating the model...\n",
      "Test Loss (Log-Transformed): 2.1366\n",
      "Testing MSE (Original Scale): 3510790648778.0547\n",
      "Testing MAPE (Original Scale): 84.65%\n"
     ]
    }
   ],
   "source": [
    "#run it on the normal X data\n",
    "print(\"Running experiment on normal X data...\")\n",
    "run_exp(X_train, y_log_train, X_test, y_log_test, input_size=X_train.shape[1], epochs=10000)\n",
    "\n",
    "#as per the tangent, try and run on the simplified X data\n",
    "print(\"\\nRunning experiment on simplified X data...\")\n",
    "run_exp(X_train_simplified, y_train_simplified_log, X_test_simplified, y_test_simplified_log, input_size=X_train_simplified.shape[1], epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95074ce4",
   "metadata": {},
   "source": [
    "As we can see, the results that we achieved from our base Neural Network model weren't as lucrative as we initially believed. Using the normal/expanded X data from before, our MAPE and MSE were much higher than anything we could reasonably want (it was worse than the base SGD regression model). We did, however, get a reduction on the simplified X data, from the simplified $108.45\\%$ to now $84.65\\%$, but that's still not as good as we want to achieve.\n",
    "\n",
    "Before brushing off neural networks completely, let's write a slightly more complicated model, where we treat the features as their own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79254ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelinedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, thumbnail_input_size, video_input_size):\n",
    "        super(PipelinedNeuralNetwork, self).__init__()\n",
    "\n",
    "        #in here, we want two pipelines, one for the thumbnail images and one for the video features\n",
    "        self.thumbnail_pipeline = nn.Sequential(\n",
    "            nn.Linear(thumbnail_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.video_pipeline = nn.Sequential(\n",
    "            nn.Linear(video_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #final output layer to combine the two pipelines\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(64+64, 64), #combine the outputs and merge to 64\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is the input data, which we split into two parts\n",
    "        #NOTE: this assumes that we are working with the simplified data, otherwise we would have to alter this structure\n",
    "        thumbnail_features = x[:, :-3] #thumbnail features is everything except for the last 3 columns (video features)\n",
    "        video_features = x[:, -3:] #video features is the last 3 columns (video features)\n",
    "\n",
    "        #pass the thumbnail features through the thumbnail pipeline\n",
    "        x_thumbnail = self.thumbnail_pipeline(thumbnail_features)\n",
    "\n",
    "        #pass the video features through the video pipeline\n",
    "        x_video = self.video_pipeline(video_features)\n",
    "\n",
    "        #combine the outputs of both pipelines\n",
    "        x_combined = torch.cat((x_thumbnail, x_video), dim=1)\n",
    "\n",
    "        #pass the combined output through the final output layer\n",
    "        return self.output_layer(x_combined)\n",
    "    \n",
    "\n",
    "#function to run the experiment with the pipelined model\n",
    "def run_pipelined_exp(X_train, y_train, X_test, y_test, thumbnail_input_size, video_input_size, epochs, batch_size=32):\n",
    "    #prepare data loaders\n",
    "    train_loader = prepare_data(X_train, y_train, batch_size)\n",
    "    test_loader = prepare_data(X_test, y_test, batch_size)\n",
    "\n",
    "    #initialize the model, optimizer, and loss function\n",
    "    model = PipelinedNeuralNetwork(thumbnail_input_size, video_input_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  #using ADAM optimizer\n",
    "    criterion = nn.MSELoss() #focusing on MSE loss\n",
    "\n",
    "    #train the model\n",
    "    print(\"Training the pipelined model...\")\n",
    "    train_model(model, optimizer, criterion, train_loader, epochs)\n",
    "\n",
    "    #evaluate the model\n",
    "    print(\"Evaluating the pipelined model...\")\n",
    "    test_loss, y_true, y_pred = evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "    #reverse log transformation for predictions\n",
    "    y_pred_original = np.expm1((np.array(y_pred) * y_std) + y_mean)\n",
    "    y_true_original = np.expm1((np.array(y_true) * y_std) + y_mean)\n",
    "\n",
    "    #clip values to avoid invalid numbers\n",
    "    y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "    y_true_original = np.clip(y_true_original, 0, 1e10)\n",
    "\n",
    "    #compute MSE and MAPE on the original scale\n",
    "    mse_original_scale = np.mean((y_true_original - y_pred_original) ** 2)\n",
    "    mape_original_scale = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100\n",
    "\n",
    "    print(f\"Test Loss (Log-Transformed): {test_loss:.4f}\")\n",
    "    print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "    print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8765bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on pipelined model with simplified X data...\n",
      "Training the pipelined model...\n",
      "Epoch 10000/10000, Loss: 0.0188\n",
      "Evaluating the pipelined model...\n",
      "Test Loss (Log-Transformed): 1.6518\n",
      "Testing MSE (Original Scale): 3895422080637.0547\n",
      "Testing MAPE (Original Scale): 99.81%\n"
     ]
    }
   ],
   "source": [
    "#now let's actually test it on the simplified data (since that is where we saw much improvement)\n",
    "thumbnail_input_size = X_train_simplified.shape[1] - 3  #all columns except the last 3\n",
    "video_input_size = 3\n",
    "\n",
    "\n",
    "print(\"Running experiment on pipelined model with simplified X data...\")\n",
    "run_pipelined_exp(X_train_simplified, y_train_simplified_log, X_test_simplified, y_test_simplified_log, thumbnail_input_size, video_input_size, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac249e1",
   "metadata": {},
   "source": [
    "We did get a worse result using the pipelined neural network, comparing the MAPE of ~84% to now ~99%. So maybe the answer is that we should have the model decide a lot more on how it should interpret itself, versus us stating what exactly it should fall into. As our last hoorah with this, let's try a state of the art model, a transformer, and see how well it performs in comparison!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f202d7",
   "metadata": {},
   "source": [
    "## 4.4) Transformer Model\n",
    "\n",
    "Surprise surprise, what would an AI project nowadays be without using a transformer somewhere! We'll again be using the simplified data, since that gives us a better MSE/MAPE overall, so let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "#using a transformer model for regression\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, nhead=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        #check that the input size is divisible by the number of heads\n",
    "        assert input_size % nhead == 0, \"input_size must be divisible by nhead\"\n",
    "        \n",
    "        #transformer encoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=input_size,  #input size\n",
    "            nhead=nhead,            #number of attention heads\n",
    "            num_encoder_layers=2,  #number of encoder layers\n",
    "            dim_feedforward=hidden_size,  #feedforward hidden size\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        #fully connected layers for regression\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0) #add an extra dimension for sequence length\n",
    "        x = self.transformer(x, x)  #pass through transformer\n",
    "        x = x.mean(dim=0)  #aggregate sequence dimension\n",
    "        return self.fc(x)\n",
    "    \n",
    "#same idea as before, but now using the transformer model\n",
    "def run_transformer_experiment(X_train, y_train, X_test, y_test, input_size, epochs=100, batch_size=32):\n",
    "    #prep data loaders\n",
    "    train_loader = prepare_data(X_train, y_train, batch_size)\n",
    "    test_loader = prepare_data(X_test, y_test, batch_size)\n",
    "\n",
    "    #initialize the model, optimizer, and loss function\n",
    "    model = TransformerModel(input_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    #train the model\n",
    "    print(\"Training the Transformer model...\")\n",
    "    train_model(model, optimizer, criterion, train_loader, epochs)\n",
    "\n",
    "    #evaluate the model\n",
    "    print(\"Evaluating the Transformer model...\")\n",
    "    test_loss, y_true, y_pred = evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "    #reverse log transformation for predictions\n",
    "    y_pred_original = np.expm1((np.array(y_pred) * y_std_simplified) + y_mean_simplified)\n",
    "    y_true_original = np.expm1((np.array(y_true) * y_std_simplified) + y_mean_simplified)\n",
    "\n",
    "    #clip values to avoid invalid numbers\n",
    "    y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "    y_true_original = np.clip(y_true_original, 0, 1e10)\n",
    "\n",
    "    #compute MSE and MAPE on the original scale\n",
    "    mse_original_scale = np.mean((y_true_original - y_pred_original) ** 2)\n",
    "    mape_original_scale = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100\n",
    "\n",
    "    print(f\"Test Loss (Log-Transformed): {test_loss:.4f}\")\n",
    "    print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "    print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bbad9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Transformer experiment on simplified X data...\n",
      "Training the Transformer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000/10000, Loss: 0.0445\n",
      "Evaluating the Transformer model...\n",
      "Test Loss (Log-Transformed): 1.3733\n",
      "Testing MSE (Original Scale): 4025449123910.6709\n",
      "Testing MAPE (Original Scale): 153.36%\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Transformer experiment on simplified X data...\")\n",
    "\n",
    "#in this, we have to pad the X values such that we can use 4 attention heads in the transformer\n",
    "#this means that the input size must be divisible by 4 (the number of heads)\n",
    "#the current size of X_train_simplified.shape[1] is 83, so we can pad it to 84 (83 + 1)\n",
    "\n",
    "X_train_simplified_padded = np.pad(X_train_simplified, ((0, 0), (0, 1)), mode='constant', constant_values=0)\n",
    "X_test_simplified_padded = np.pad(X_test_simplified, ((0, 0), (0, 1)), mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "run_transformer_experiment(\n",
    "    X_train_simplified_padded, y_train_simplified_log,\n",
    "    X_test_simplified_padded, y_test_simplified_log,\n",
    "    input_size=X_train_simplified_padded.shape[1],\n",
    "    epochs=10000,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679190f4",
   "metadata": {},
   "source": [
    "That was kind of dissapointing, our previous neural network (the base version) got a MAPE of ~84% in less than a minute, but this transformer, after training for around 33 minutes, got a MAPE of 153%. There might be a better result if we were to combine it together or gain a bunch more data, but it seems as though we are lacking on both time and information that we can reasonably extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbb588",
   "metadata": {},
   "source": [
    "## 4.5) New Regressor: XGBoost Regressor\n",
    "\n",
    "After recommendations by colleagues, I found another model that could prove to be useful, it's called the XGBoost regressor! We will train it on both the expanded and simplified X data, and see the improvements or losses that we achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f329ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Running XGBoost experiment on expanded data...\n",
      "Training the XGBoost model...\n",
      "Evaluating the XGBoost model...\n",
      "Testing MSE (Original Scale): 3323310021618.3706\n",
      "Testing MAPE (Original Scale): 112.12%\n",
      "\n",
      "Running XGBoost experiment on simplified data...\n",
      "Training the XGBoost model...\n",
      "Evaluating the XGBoost model...\n",
      "Testing MSE (Original Scale): 3094432857981.5986\n",
      "Testing MAPE (Original Scale): 102.28%\n"
     ]
    }
   ],
   "source": [
    "%pip install -q xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "#function to run the XGBoost experiment (generalized again so we don't have to repeat code between expanded and simplified)\n",
    "def run_xgboost_experiment(X_train, y_train, X_test, y_test):\n",
    "    #create the regressor itself\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    #train the model\n",
    "    print(\"Training the XGBoost model...\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #predict on the test set\n",
    "    print(\"Evaluating the XGBoost model...\")\n",
    "    y_pred_log = xgb_model.predict(X_test)\n",
    "\n",
    "    #reverse log transformation for predictions\n",
    "    y_pred_original = np.expm1((y_pred_log * y_std) + y_mean)\n",
    "    y_test_original = np.expm1((y_test * y_std) + y_mean)\n",
    "\n",
    "    #clip values to avoid invalid numbers\n",
    "    y_pred_original = np.clip(y_pred_original, 0, 1e10)\n",
    "    y_test_original = np.clip(y_test_original, 0, 1e10)\n",
    "\n",
    "    #compute MSE and MAPE on the original scale\n",
    "    mse_original_scale = mean_squared_error(y_test_original, y_pred_original)\n",
    "    mape_original_scale = mean_absolute_percentage_error(y_test_original, y_pred_original) * 100\n",
    "\n",
    "    print(f\"Testing MSE (Original Scale): {mse_original_scale:.4f}\")\n",
    "    print(f\"Testing MAPE (Original Scale): {mape_original_scale:.2f}%\")\n",
    "\n",
    "#now lets run it on the expanded and simplified data\n",
    "print(\"Running XGBoost experiment on expanded data...\")\n",
    "run_xgboost_experiment(\n",
    "    X_train, y_log_train,\n",
    "    X_test, y_log_test,\n",
    ")\n",
    "\n",
    "print(\"\\nRunning XGBoost experiment on simplified data...\")\n",
    "run_xgboost_experiment(\n",
    "    X_train_simplified, y_train_simplified_log,\n",
    "    X_test_simplified, y_test_simplified_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e422167",
   "metadata": {},
   "source": [
    "This did give us a new solid prediction mechanism, with a MAPE of $102.28\\%$, so let's use this, in combination with others, to hopefully get our final (and best) result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c84ab3",
   "metadata": {},
   "source": [
    "## 4.6) Last effort: Stacking Ensemble\n",
    "\n",
    "For this last effort, we will be using the models with the best results that we have right now, the `BaseCustomNNModel`, the `XGBRegressor` model, and the SkLearn `SGDRegressor` model, and we will be stacking them and using them ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0135906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BaseCustomNNModel...\n",
      "Epoch 10000/10000, Loss: 0.0240\n",
      "Training XGBoost model...\n",
      "Training SGDRegressor model...\n",
      "BaseCustomNNModel - MSE (Original Scale): 6649865407417.0986\n",
      "XGBoost - MSE (Original Scale): 6649864509330.7334\n",
      "SGDRegressor - MSE (Original Scale): 6649864935004.3125\n",
      "Ensemble Model - MSE (Original Scale): 2959120555250.2793\n",
      "Ensemble Model - MAPE (Original Scale): 106.24%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "#BaseCustomNNModel\n",
    "print(\"Training BaseCustomNNModel...\")\n",
    "train_loader = prepare_data(X_train_simplified, y_train_simplified_log)\n",
    "test_loader = prepare_data(X_test_simplified, y_test_simplified_log)\n",
    "input_size = X_train_simplified.shape[1]  #number of features in the simplified data\n",
    "\n",
    "#initialize and run the model (as we did before)\n",
    "model = BaseCustomNNModel(input_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "train_model(model, optimizer, criterion, train_loader, 10000)\n",
    "_, _, y_pred_nn_train = evaluate_model(model, criterion, train_loader) #evaluate for the training set\n",
    "_, _, y_pred_nn = evaluate_model(model, criterion, test_loader) #evaluate for the test set\n",
    "\n",
    "#convert the predictions to numpy arrays\n",
    "nn_predictions_train = np.array(y_pred_nn_train)\n",
    "nn_predictions_test = np.array(y_pred_nn)\n",
    "\n",
    "#xgb model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train_simplified, y_train_simplified_log)\n",
    "xgb_predictions_train = xgb_model.predict(X_train_simplified)\n",
    "xgb_predictions_test = xgb_model.predict(X_test_simplified)\n",
    "\n",
    "#sklearn model (with optimal hyperparameters)\n",
    "print(\"Training SGDRegressor model...\")\n",
    "sgd_model = SGDRegressor(\n",
    "    loss='epsilon_insensitive',\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,\n",
    "    alpha=0.08889,\n",
    "    max_iter=1000,\n",
    "    epsilon=0.46,\n",
    "    l1_ratio=0.0,\n",
    "    penalty='l2',\n",
    "    random_state=42\n",
    ")\n",
    "sgd_model.fit(X_train_simplified, y_train_simplified_log)\n",
    "sgd_predictions_train = sgd_model.predict(X_train_simplified)\n",
    "sgd_predictions_test = sgd_model.predict(X_test_simplified)\n",
    "\n",
    "\n",
    "#========== Now that we have the predictions, we can stack them ==========\n",
    "train_predictions = np.vstack([nn_predictions_train, xgb_predictions_train, sgd_predictions_train]).T\n",
    "test_predictions = np.vstack([nn_predictions_test, xgb_predictions_test, sgd_predictions_test]).T\n",
    "\n",
    "\n",
    "#========== Now we can train the meta-model on the stacked predictions ==========\n",
    "#in this case, we will use a linear regression model as the meta-model\n",
    "#this is a simple choice, but we can also use more complex models if needed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(train_predictions, y_train_simplified_log)\n",
    "\n",
    "#predict and evaluate\n",
    "ensemble_predictions_log = meta_model.predict(test_predictions)\n",
    "ensemble_predictions_original = np.expm1((ensemble_predictions_log * y_std_simplified) + y_mean_simplified)\n",
    "y_test_original = np.expm1((y_test_simplified_log * y_std_simplified) + y_mean_simplified)\n",
    "\n",
    "#clipping as usual\n",
    "ensemble_predictions_original = np.clip(ensemble_predictions_original, 0, 1e10)\n",
    "y_test_original = np.clip(y_test_original, 0, 1e10)\n",
    "\n",
    "#compute and print metrics\n",
    "mse_ensemble = mean_squared_error(y_test_original, ensemble_predictions_original)\n",
    "mape_ensemble = mean_absolute_percentage_error(y_test_original, ensemble_predictions_original) * 100\n",
    "\n",
    "#print intermediary results\n",
    "print(f\"BaseCustomNNModel - MSE (Original Scale): {mean_squared_error(y_test_original, nn_predictions_test):.4f}\")\n",
    "print(f\"XGBoost - MSE (Original Scale): {mean_squared_error(y_test_original, xgb_predictions_test):.4f}\")\n",
    "print(f\"SGDRegressor - MSE (Original Scale): {mean_squared_error(y_test_original, sgd_predictions_test):.4f}\")\n",
    "\n",
    "print(f\"Ensemble Model - MSE (Original Scale): {mse_ensemble:.4f}\")\n",
    "print(f\"Ensemble Model - MAPE (Original Scale): {mape_ensemble:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758ead2",
   "metadata": {},
   "source": [
    "I feel like we could keep going down this rabbithole of varied combinations and values, but in the end, we got a result that was better than randomly picking data. Our best result was still the `BaseCustomNNModel` by itself, with a MAPE of 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e0fd3",
   "metadata": {},
   "source": [
    "## 5.) Conclusions about this project\n",
    "\n",
    "Unsurprisingly, it is difficult for us to determine what makes a good youtube video! We started by working with all data features that we could retrieve, and we slowly simplified our problem down further and further into managable chunks. These chunks lead us into places like \"What does the user actually see/interact with\" and \"What keeps a user invested in the video\".\n",
    "\n",
    "There could have been several improvements made on this data, such as including every single youtube video (trending or not), including channel data, including data about when it was posted (and how it compared with trends in that month), and so on. However, we can probably state that there is no \"one size fits all\" for what becomes trending or not, as it depends on much more factors than simply the singular video that is uploaded.\n",
    "\n",
    "Honestly, I wish that I had more time to work on this project, as I could continue going through and adding/altering data, more trending information, more everything; but we simply don't have enough time to cover all of the possibilities nor the resources to try and <strong>essentially reverse engineer the youtube recommendation/trending algorithm</strong>. It sucks that we couldn't find a better option than \"going with the flow of youtube\", but at least we made some progress in understanding that it's not as simple as \"cat+dog=100M views\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
